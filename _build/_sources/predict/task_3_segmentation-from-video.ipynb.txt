{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1 a) Process Quadrat in MP4 Video\n",
    "This notebook is configured to read a video specified by `file`, then extract the lines that compose the quadrat and estimate the appropriate corner points for cropping its \n",
    "contents. The output is a lower resolution MP4 video that is annotated with the found lines \n",
    "and corner points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if notebook is running in Colab or local workstation\n",
    "import sys\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Search for all video files on Google Drive...\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    DATA_PATH = r'/content/drive/My Drive/Data'\n",
    "    \n",
    "    # cd into git repo so python can find utils\n",
    "    %cd '/content/drive/My Drive/cciw-zebra-mussel'\n",
    "    \n",
    "    # clone repo, install packages\n",
    "else:\n",
    "    DATA_PATH = r'/scratch/gallowaa/cciw/Data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from glob import glob\n",
    "#from utils import draw_lines\n",
    "\n",
    "# https://ipython.org/ipython-doc/3/config/extensions/autoreload.html\n",
    "#%load_ext autoreload\n",
    "#%autoreload 2\n",
    "\n",
    "figsize = (8, 6)\n",
    "save_figures = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "\n",
    "#import glob\n",
    "\n",
    "# for manually reading high resolution images\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# for comparing predictions to lab analysis data frames\n",
    "import pandas as pd\n",
    "\n",
    "# for plotting\n",
    "import matplotlib\n",
    "# enable LaTeX style fonts\n",
    "matplotlib.rc('text', usetex=True)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rc('text', usetex=True)\n",
    "plt.rc('font', family='serif')\n",
    "\n",
    "# pytorch core library\n",
    "import torch\n",
    "# pytorch neural network functions\n",
    "from torch import nn\n",
    "# pytorch dataloader\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# for post-processing model predictions by conditional random field \n",
    "import pydensecrf.densecrf as dcrf\n",
    "import pydensecrf.utils as utils\n",
    "\n",
    "from tqdm import tqdm  # progress bar\n",
    "\n",
    "# evaluation metrics\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import jaccard_score as jsc\n",
    "\n",
    "# local imports (files provided by this repo)\n",
    "#import transforms as T\n",
    "\n",
    "# various helper functions, metrics that can be evaluated on the GPU\n",
    "#from task_3_utils import evaluate, evaluate_loss, eval_binary_iou, pretty_image\n",
    "\n",
    "# Custom dataloader for rapidly loading images from a single LMDB file\n",
    "#from folder2lmdb import VOCSegmentationLMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "#from utils import draw_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fcn import FCN8slim\n",
    "net = FCN8slim(n_class=1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    root = osp.join(DATA_PATH, 'Checkpoints/fcn8slim_lr1e-03_wd5e-04_bs32_ep50_seed1')\n",
    "else:\n",
    "    #root = '/scratch/gallowaa/cciw/logs/lab-v1.0.0/fcn8slim/lr1e-03/wd5e-04/bs32/ep50/seed1/checkpoint' # a\n",
    "    #root = '/scratch/gallowaa/cciw/logs/v1.0.1-debug/fcn8s/lr1e-03/wd5e-04/bs25/ep80/seed4/checkpoint' # b\n",
    "    #root = '/scratch/gallowaa/cciw/logs/v1.1.0-debug/fcn8s/lr1e-03/wd5e-04/bs25/ep80/seed9/checkpoint/' # c\n",
    "    #root = '/scratch/gallowaa/cciw/logs/v111/trainval/fcn8s/lr1e-03/wd5e-04/bs40/ep80/seed2/checkpoint/' # d\n",
    "    #root = '/scratch/gallowaa/cciw/logs/v111/trainval/fcn8slim/lr1e-04/wd5e-04/bs40/ep80/seed1/checkpoint/' # e\n",
    "    root = '/scratch/gallowaa/cciw/logs/cmp-dataset/train_v120/deeplabv3_resnet50/lr1e-01/wd5e-04/bs40/ep80/seed3/checkpoint'\n",
    "    #root = '/scratch/gallowaa/cciw/logs/cmp-dataset/trainval_v120/fcn8slim/lr1e-03/wd5e-04/bs50/ep80/seed1/checkpoint/'\n",
    "    #root = '/scratch/gallowaa/cciw/logs/cmp-dataset/trainval_v120/fcn8slim/lr1e-03/wd5e-04/bs50/ep80/seed1/checkpoint/'\n",
    "\n",
    "#ckpt_file = 'fcn8slim_lr1e-03_wd5e-04_bs32_ep50_seed1_epoch40.ckpt' # a\n",
    "#ckpt_file = 'fcn8s_lr1e-03_wd5e-04_bs25_ep80_seed4_epoch70.ckpt' # b\n",
    "#ckpt_file = 'fcn8s_lr1e-03_wd5e-04_bs25_ep80_seed9_epoch10.ckpt'\n",
    "#ckpt_file = 'fcn8s_lr1e-03_wd5e-04_bs40_ep80_seed2amp_epoch79.pt' # d\n",
    "#ckpt_file = 'fcn8slim_lr1e-04_wd5e-04_bs40_ep80_seed1amp_epoch79.pt' # e\n",
    "ckpt_file = 'deeplabv3_resnet50_lr1e-01_wd5e-04_bs40_ep80_seed3_epoch79.ckpt'\n",
    "#ckpt_file = 'fcn8slim_lr1e-03_wd5e-04_bs50_ep80_seed1_epoch79.pt'\n",
    "#ckpt_file = 'fcn8slim_lr1e-03_wd5e-04_bs50_ep80_seed1amp_epoch79.pt'\n",
    "\n",
    "\"\"\"Feel free to try these other checkpoints later after running epoch40 to get a \n",
    "feel for how the evaluation metrics change when model isn't trained as long.\"\"\"\n",
    "#ckpt_file = 'fcn8slim_lr1e-03_wd5e-04_bs32_ep50_seed1_epoch10.ckpt'\n",
    "#ckpt_file = 'fcn8slim_lr1e-03_wd5e-04_bs32_ep50_seed1_epoch0.ckpt'\n",
    "\n",
    "checkpoint = torch.load(osp.join(root, ckpt_file))\n",
    "train_loss = checkpoint['trn_loss']\n",
    "val_loss = checkpoint['val_loss']\n",
    "print('==> Resuming from checkpoint..')\n",
    "net = checkpoint['net']\n",
    "\n",
    "# AMP\n",
    "#net.load_state_dict(checkpoint['net'])\n",
    "#amp.load_state_dict(checkpoint['amp'])\n",
    "last_epoch = checkpoint['epoch'] + 1\n",
    "torch.set_rng_state(checkpoint['rng_state'])\n",
    "\n",
    "# later appended to figure filenames\n",
    "model_stem = ckpt_file.split('.')[0]\n",
    "\n",
    "print('Loaded model %s trained to epoch ' % model_stem, last_epoch)\n",
    "print('Cross-entropy loss {:.4f} for train set, {:.4f} for validation set'.format(train_loss, val_loss))\n",
    "\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apex import amp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Set to True to save the model predictions in PNG format, \n",
    "otherwise proceed to predict biomass without saving images\"\"\"\n",
    "SAVE_PREDICTIONS = True\n",
    "\n",
    "if SAVE_PREDICTIONS:\n",
    "    prediction_path = ''\n",
    "    for t in root.split('/')[:-1]:\n",
    "        prediction_path += t + '/'\n",
    "\n",
    "    prediction_path = osp.join(prediction_path, 'videos')\n",
    "\n",
    "    if not osp.exists(prediction_path):\n",
    "        os.mkdir(prediction_path)\n",
    "    \n",
    "print(prediction_path)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apex import amp\n",
    "net = amp.initialize(net, opt_level='O3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segmentation(im):\n",
    "    img = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
    "    img = img / 255.\n",
    "    img = ((img - np.array([0.5, 0.5, 0.5])) / np.array([0.5, 0.5, 0.5]))\n",
    "    imgt = torch.FloatTensor(img).to(device)\n",
    "    imgt = imgt.unsqueeze(0)\n",
    "    # Note: need to call contigious after the permute \n",
    "    # else max pooling will fail\n",
    "    nchw_tensor = imgt.permute(0, 3, 1, 2).contiguous()\n",
    "    with torch.no_grad():\n",
    "        pred = sig(net(nchw_tensor)['out'])\n",
    "    pred_np = pred.detach().cpu().numpy().squeeze()    \n",
    "    return pred_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig = nn.Sigmoid()  # initializes a sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file = 'GLNI_456-CloseUp_2016-07-11_video-1.mp4'\n",
    "#file = 'GLNI_456-2_2014-05-27_video-1.mp4'\n",
    "\n",
    "#file = 'GLNI_456-1_2014-05-27_video-1.mp4'\n",
    "#file = 'GLNI_1208-2_2014-08-27_video-1.mp4'\n",
    "#file = 'GLNI_1208-1_2014-08-27_video-1.mp4'\n",
    "#file = 'GLNI_1208-2_2014-06-04_video-2.mp4'\n",
    "file = 'GLNI_1342-1_2014-08-21_video-1.mp4'\n",
    "#file = 'GLNI_1342-3_2014-09-25_video-1.mp4'\n",
    "#file = 'GLNI_1342-2_2014-09-25_video-1.mp4'\n",
    "#file = 'GLNI_1342-2_2014-05-28_video-1.mp4'\n",
    "#file = 'GLNI_1354-1_2014-05-28_video-1.mp4'\n",
    "\n",
    "#file = 'GLNI_1347-3_2013-09-13_video-1.mp4'\n",
    "#file = 'GLNI_1347-2_2013-09-13_video-1.mp4'\n",
    "#file = 'GLNI_1347-1_2013-09-13_video-1.mp4'\n",
    "\n",
    "#file = 'GLNI_503-1_2013-05-08_video-1.mp4'\n",
    "#file = 'GLNI_1347-2_2013-05-01_video-1.mp4'\n",
    "#file = 'GLNI_1347-3_2016-07-05_video-1.mp4'\n",
    "#file = 'GLNI_1347-1_2016-07-05_video-1.mp4'\n",
    "#file = 'GLNI_12-3_2016-07-11_video-1.mp4'\n",
    "#file = 'GLNI_12-1_2016-07-11_video-1.mp4'\n",
    "#file = 'GLNI_12-1_2016-07-11_video-1.mp4'\n",
    "#file = 'GLNI_456-3_2015-07-17_video-1.mp4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_videos[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_videos = glob(os.path.join(DATA_PATH, 'Videos_and_stills/GLNI/*/*/*/Videos/Quad*/*.mp4'))\n",
    "all_videos = glob(os.path.join(DATA_PATH, 'Videos_and_stills/GLNI/*/*/*/Videos/CloseUp/*.mp4'))\n",
    "videotable_path = os.path.join(DATA_PATH, 'Tables', 'QuadratVideos.csv')\n",
    "video_df = pd.read_csv(videotable_path, index_col=0)\n",
    "\n",
    "vpath = video_df.iloc[video_df[video_df['Name'] == file].index]['Quadrat Video Path']\n",
    "tokens = video_df[video_df['Name'] == file]['Quadrat Video Path'].values[0].split('\\\\')\n",
    "\n",
    "video_path = DATA_PATH + '/Videos_and_stills/GLNI'\n",
    "for tok in tokens[4:-1]:\n",
    "    video_path += '/' + tok\n",
    "    \n",
    "video_path = os.path.join(video_path, file)\n",
    "print('Loading video: ', video_path)\n",
    "#all_videos[-2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"These are meta-parameters of the Probabilistic Hough Line Transform, \n",
    "note there are additional params in cell 6 which we set according to \n",
    "the input resolution.\n",
    "\n",
    "@param rho Distance resolution of the accumulator (pixels)\"\"\"\n",
    "rho = 1  \n",
    "\n",
    "\"\"\"\n",
    "@param theta Angle resolution of the accumulator (radians)\n",
    "\n",
    "Suggest to use a value no less than np.pi/90 else too many \n",
    "spurious lines will be found. theta=np.pi/45 means lines \n",
    "must differ by 4 degrees to be considered distinct.\"\"\"\n",
    "theta = np.pi / 45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file\n",
    "\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "sz = (int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),\n",
    "      int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)))\n",
    "print('Raw input resolution', sz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read first frame to adjust resolution of output stream\n",
    "ret, im = cap.read() \n",
    "\n",
    "# set additional meta-parameters according to input res\n",
    "if sz[0] == 1440:\n",
    "    \"\"\"x_trim and y_trim are used to remove black padding \n",
    "    which triggers spurious edges\"\"\"\n",
    "    x_trim, y_trim = 1, 145\n",
    "    im = im[y_trim:-y_trim, x_trim:-x_trim, :]\n",
    "    crop_frame_border = True\n",
    "    '''\n",
    "    \"\"\"@param canny_thresh# hysteresis values for Canny edge \n",
    "    detector, input to HoughLines\"\"\"\n",
    "    canny_thresh1, canny_thresh2 = 60, 300\n",
    "    \n",
    "    \"\"\"@param threshold Accumulator threshold, return \n",
    "    lines with more than threshold of votes. (intersection points)\"\"\"\n",
    "    threshold = 125\n",
    "    \n",
    "    \"\"\"@param minLineLength Minimum line length. \n",
    "    Line segments shorter than that are rejected. (pixels)\"\"\"\n",
    "    mLL = 400\n",
    "    \n",
    "    \"\"\"@param maxLineGap Maximum allowed gap between points \n",
    "    on the same line to link them. (pixels)\"\"\"\n",
    "    mLG = 150\n",
    "    '''\n",
    "else:\n",
    "    # params as described above\n",
    "    canny_thresh1, canny_thresh2 = 30, 300\n",
    "    threshold = 125\n",
    "    mLG, mLL = 250, 600\n",
    "    crop_frame_border = False\n",
    "\n",
    "\"\"\"this method may downsample, so set the video writer \n",
    "resolution to the processed image resolution\"\"\"\n",
    "'''\n",
    "img, edges = draw_lines(im, rho=rho, theta=theta, mll=mLL, \n",
    "                        mlg=mLG, threshold=threshold, ds=1)    \n",
    "'''                    \n",
    "sz = (im.shape[1], im.shape[0])\n",
    "print(sz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#osp.join(prediction_path, file.split('.')[0] + '_' + model_stem + '-quadrat-demo.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cap.isOpened():\n",
    "    fps = 20\n",
    "    vout = cv2.VideoWriter()\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    outfile = file.split('.')[0] + '_' + model_stem + '-quadrat-demo.mp4'\n",
    "    vout.open(osp.join(prediction_path, outfile), fourcc, fps, sz, True)\n",
    "    print('Opened stream for writing, output resolution is', sz)\n",
    "else:\n",
    "    print('cap is not open')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Confim that this cell prints \"Found GPU, cuda\". If not, select \"GPU\" as \n",
    "\"Hardware Accelerator\" under the \"Runtime\" tab of the main menu.\n",
    "\"\"\"\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print('Found GPU,', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "currentFrame = 0\n",
    "\n",
    "\"\"\"it can take 30s-1min to process entire video, \n",
    "can optionally process a small number of frames\"\"\"\n",
    "\n",
    "'''\n",
    "for _ in range(10):\n",
    "    # Capture frame-by-frame\n",
    "    ret, im = cap.read()\n",
    "    if not ret: break\n",
    "'''\n",
    "\n",
    "# to process whole video    \n",
    "#for _ in range(100):\n",
    "while(True):\n",
    "    \n",
    "    # Capture frame-by-frame\n",
    "    ret, im = cap.read()\n",
    "    if not ret: break\n",
    "\n",
    "    '''\n",
    "    # For saving still images\n",
    "    name = 'pframe_pi45_' + str(currentFrame) + '.jpg'\n",
    "    save_path = os.path.join(out_path, name)\n",
    "    print ('Creating...' + name)\n",
    "    '''\n",
    "    \n",
    "    if crop_frame_border:\n",
    "        im = im[y_trim:-y_trim, x_trim:-x_trim, :]\n",
    "        \n",
    "    pred_np = segmentation(im)\n",
    "    \n",
    "    # Do processing\n",
    "    '''\n",
    "    img, _ = draw_lines(im, rho=rho, theta=theta, mll=mLL, \n",
    "                        mlg=mLG, threshold=threshold, ds=1, \n",
    "                        canny_1=canny_thresh1, canny_2=canny_thresh2)\n",
    "    '''\n",
    "    \n",
    "    \"\"\"Save still image in jpeg format\n",
    "    cv2.imwrite(save_path, img)\"\"\"\n",
    "    \n",
    "    \"\"\"For annotating video\n",
    "    @param org Bottom-left corner of the text string (default=50).\n",
    "    @param org Bottom-left corner of the text string in the image (default=50).\n",
    "    @param fontFace Font type, see #HersheyFonts (default=cv2.FONT_HERSHEY_PLAIN).\n",
    "    @param fontScale Font scale factor that is multiplied \n",
    "                     by the font-specific base size (default=2).\n",
    "    @param color Text color (default=(R=0, G=255, B=0)).\n",
    "    @param thickness Thickness of the lines used to draw a text (default=1).\n",
    "    @param lineType Line type. See #LineTypes (default=cv2.LINE_AA).\"\"\"\n",
    "    '''\n",
    "    cv2.putText(                 # x, y\n",
    "        img, str(currentFrame), (50, 50), cv2.FONT_HERSHEY_PLAIN, 2, (0,255,0), 1, cv2.LINE_AA)\n",
    "    '''\n",
    "    \n",
    "    #im[:, :, 0] += (pred_np * 255).astype('uint8')\n",
    "    p = (pred_np * 255).astype('uint8')\n",
    "    #p = cv2.cvtColor(p, cv2.COLOR_GRAY2RGB)\n",
    "    src2 = np.zeros((p.shape[0], p.shape[1], 3), np.uint8)\n",
    "    src2[:, :, 0] = p\n",
    "    #p[:, :, 0] = p[:, :, 0] * 2\n",
    "    dst = cv2.addWeighted(im, 0.5, src2, 0.5, 0)\n",
    "    vout.write(dst)    \n",
    "\n",
    "    # increment frame counter\n",
    "    currentFrame += 1\n",
    "\n",
    "\"\"\"When everything done, release the \n",
    "capture to flush the video stream. \n",
    "\n",
    "Before re-running this cell, first do \n",
    "cells 5, 6, and 7 to re-open cap and \n",
    "vout, otherwise ret=False and this \n",
    "cell does nothing.\"\"\"\n",
    "cap.release()\n",
    "vout.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see new mp4 files when listing current dir. You may not be able to view them directly in Google Drive,\n",
    "but these can be downloaded to your local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#net.eval()\n",
    "net.training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#im.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.imshow((pred_np * 255).astype('uint8'))\n",
    "#plt.imshow(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End Demo\n",
    "\n",
    "What follows is additional code for debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# to seek into a specific frame\n",
    "for _ in range(28):\n",
    "    ret, im = cap.read()\n",
    "\"\"\"    \n",
    "\"\"\"\n",
    "ret, im = cap.read()\n",
    "if ret is not None:\n",
    "    plt.imshow(im)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "if crop_frame_border:\n",
    "    im = im[y_trim:-y_trim, x_trim:-x_trim, :]\n",
    "\n",
    "img, edges = draw_lines(im, rho=1, theta=np.pi/45, mll=mLL,\n",
    "                        mlg=mLG, threshold=125, ds=1, \n",
    "                        canny_1=canny_thresh1, canny_2=canny_thresh2)\n",
    "'''                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(14, 12))\n",
    "#plt.imshow(img)\n",
    "#plt.imshow(im[y_trim:-y_trim, x_trim:-x_trim, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.imshow(edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"it can be helpful to save particularly difficult\n",
    "frames for processing in a different notebook\"\"\"\n",
    "#cv2.imwrite('test.jpg', im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#cv2.circle(img, (800, 400), 10, (255, 0, 0), thickness=2, lineType=8, shift=0)\n",
    "plt.figure(figsize=figsize)\n",
    "plt.imshow(img)\n",
    "plt.tight_layout()\n",
    "if save_figures:\n",
    "    plt.savefig('img/' + outpath + '-Step-2.png')\n",
    "plt.show()\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
