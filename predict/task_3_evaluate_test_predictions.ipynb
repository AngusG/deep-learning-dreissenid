{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0UXd9CuzgNx2"
   },
   "source": [
    "# Evaluate a Pre-Trained Segmentation Model in Colab on 2019 data\n",
    "\n",
    "\n",
    "This notebook is for generating predictions on the GLNI 2019 test set and evaluating the proportion of variance ($R^2$) in live coverage, biomass, and count, explained by the predictions.\n",
    "\n",
    "Since I do not have access to the lab analysis tables for the 2019 data, it currently performs the analysis on the training images as a demonstration, but in line comments are provided in order to make the transition to the 2019 data.\n",
    "\n",
    "This notebook is designed to be as lean as possible, it __does not__ aim to provide interactive visualizations, for this see other notebooks, for example: `task_3_evaluate_notebook_in_colab`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note:__ To maintain a high priority Colab user status such that sufficient GPU resources are available in the future, ensure to free the runtime when finished running this notebook. This can be done using 'Runtime > Manage Sessions' and click 'Terminate'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Check if notebook is running in Colab or local workstation\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    !ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
    "    !pip install gputil\n",
    "    !pip install psutil\n",
    "    !pip install humanize\n",
    "\n",
    "import GPUtil as GPU\n",
    "import os\n",
    "import humanize\n",
    "import psutil\n",
    "import sys\n",
    "GPUs = GPU.getGPUs()\n",
    "\n",
    "try:\n",
    "    # XXX: only one GPU on Colab and isn’t guaranteed\n",
    "    gpu = GPUs[0]\n",
    "\n",
    "    def printm():\n",
    "        process = psutil.Process(os.getpid())\n",
    "        print(\"Gen RAM Free: \" + humanize.naturalsize(psutil.virtual_memory().available),\n",
    "              \" | Proc size: \" + humanize.naturalsize(process.memory_info().rss))\n",
    "        print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(\n",
    "            gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
    "    printm()\n",
    "\n",
    "    # Check if GPU capacity is sufficient to proceed\n",
    "    if gpu.memoryFree < 5000:\n",
    "        print(\"\\nInsufficient memory! Some cells may fail. Please try restarting the runtime using 'Runtime → Restart Runtime...' from the menu bar. If that doesn't work, terminate this session and try again later.\")\n",
    "    else:\n",
    "        print('\\nGPU memory is sufficient to proceeed.')\n",
    "except:\n",
    "    print('Select the Runtime → \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
    "    print('and then re-execute this cell.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "4pzGY41DgyyQ",
    "outputId": "6bebf55b-f9f5-4a59-abdd-4c101878f4d1"
   },
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    DATA_PATH = r'/content/drive/My Drive/Data'\n",
    "\n",
    "    # cd into git repo so python can find utils\n",
    "    %cd '/content/drive/My Drive/cciw-zebra-mussel/predict'\n",
    "\n",
    "    sys.path.append('/content/drive/My Drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bneyBxcYgNx7"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "\n",
    "import glob\n",
    "\n",
    "# for manually reading high resolution images\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# for comparing predictions to lab analysis data frames\n",
    "import pandas as pd\n",
    "\n",
    "# for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# pytorch core library\n",
    "import torch\n",
    "\n",
    "# pytorch neural network functions\n",
    "from torch import nn\n",
    "\n",
    "from tqdm import tqdm_notebook  # notebook friendly progress bar\n",
    "\n",
    "# evaluation metrics\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# local plotting functions\n",
    "from plot_utils import *\n",
    "\n",
    "from task_3_utils import img_to_nchw_tensor, pretty_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Confim that this cell prints \"Found GPU, cuda\". If not, select \"GPU\" as \n",
    "\"Hardware Accelerator\" under the \"Runtime\" tab of the main menu.\n",
    "\"\"\"\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print('Found GPU,', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n4P0mldogNyQ"
   },
   "source": [
    "## 1. Load a pre-trained model checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "n89QOGLxgNyS",
    "outputId": "904c901b-f3e2-4a84-afd8-27eaad8aadf7"
   },
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    root = osp.join(\n",
    "        DATA_PATH, 'Checkpoints/deeplabv3_resnet50_lr1e-01_wd5e-04_bs40_ep80_seed1')\n",
    "    \n",
    "else:\n",
    "    root = osp.join(\n",
    "        os.environ['DATA_PATH'], 'cciw/logs/cmp-dataset/train_v120/deeplabv3_resnet50/lr1e-01/wd5e-04/bs40/ep80/seed1/checkpoint')\n",
    "\n",
    "ckpt_file = 'deeplabv3_resnet50_lr1e-01_wd5e-04_bs40_ep80_seed1_epoch79.ckpt'\n",
    "\n",
    "model_to_load = osp.join(root, ckpt_file)\n",
    "                         \n",
    "print('Loading', model_to_load)                         \n",
    "         \n",
    "checkpoint = torch.load(model_to_load)\n",
    "                        \n",
    "train_loss = checkpoint['trn_loss']\n",
    "val_loss = checkpoint['val_loss']\n",
    "print('==> Resuming from checkpoint..')\n",
    "net = checkpoint['net']\n",
    "last_epoch = checkpoint['epoch']\n",
    "torch.set_rng_state(checkpoint['rng_state'])\n",
    "\n",
    "# later appended to figure filenames\n",
    "model_stem = ckpt_file.split('.')[0]\n",
    "\n",
    "print('Loaded model %s trained to epoch ' % model_stem, last_epoch)\n",
    "print(\n",
    "    'Cross-entropy loss {:.4f} for train set, {:.4f} for validation set'.format(train_loss, val_loss))\n",
    "\n",
    "sig = nn.Sigmoid()  # initializes a sigmoid function\n",
    "\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ne_OPnTPgNzh"
   },
   "source": [
    "## 2. Set path to original, i.e., full size images\n",
    "\n",
    "Here we manually load and preprocess the original images and png masks using OpenCV.\n",
    "\n",
    "### Note to Dominique: set `root_path` to point to validation images after enabling 2019 tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "rouYvCy3gNzj",
    "outputId": "0cd6d473-73c3-4f2e-97ca-df03fbb55f7a"
   },
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    # uncomment me after enabling 2019 table!\n",
    "    #root_path = osp.join(DATA_PATH, 'ADIG_Labelled_Dataset/val_v101/GLNI') \n",
    "    root_path = osp.join(DATA_PATH, 'ADIG_Labelled_Dataset/train_v120/')\n",
    "else:\n",
    "    #root_path = '/scratch/ssd/gallowaa/cciw/VOCdevkit/Validation-v101-originals/'\n",
    "    root_path = '/scratch/ssd/gallowaa/cciw/VOCdevkit/Train-v112-originals/'\n",
    "\n",
    "jpeg_files = glob.glob(osp.join(root_path, 'JPEGImages/') + '*.jpg')\n",
    "png_files = glob.glob(osp.join(root_path, 'SegmentationClass/') + '*_crop.png')\n",
    "\n",
    "jpeg_files.sort()\n",
    "png_files.sort()\n",
    "\n",
    "\"\"\"\n",
    "Should equal 121 for train-v112\n",
    "Should equal 151 for train-v112\n",
    "Should equal  55 for val-v101\n",
    "Should equal  36 for v1.0.0 Lab\"\"\"\n",
    "print(len(jpeg_files))\n",
    "print(len(png_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fontsize = 16\n",
    "\n",
    "left = 0.02  # the left side of the subplots of the figure\n",
    "right = 0.98  # the right side of the subplots of the figure\n",
    "bottom = 0.05  # the bottom of the subplots of the figure\n",
    "top = 0.95  # the top of the subplots of the figure\n",
    "wspace = 0.15  # the amount of width reserved for space between subplots, expressed as a fraction of the average axis width\n",
    "hspace = 0.1   # the amount of height reserved for space between subplots, expressed as a fraction of the average axis height\n",
    "\n",
    "BIOMASS_IDX = 0\n",
    "COUNT_IDX = 1\n",
    "LIVE_COV_IDX = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9h2AhUsjgNz-"
   },
   "source": [
    "## 3. Populate dataframe of lab measurements associated with `jpeg_files`\n",
    "\n",
    "Here we predict the mussel biomass from the lab analysis using a) the masks, and b) model predictions on the \n",
    "full size images. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relates to Deliverable 2. c) *Predicted semantic segmentation (mussel/no-mussel) for all images in 2019 testing set in png image format.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vYuNMNd9gNz_"
   },
   "outputs": [],
   "source": [
    "if not IN_COLAB:\n",
    "\n",
    "    # set DATA_PATH local machine\n",
    "    DATA_PATH = osp.join(os.environ['DATA_PATH'], 'cciw/Data')\n",
    "    \n",
    "    # enable LaTeX style fonts only on local workstation\n",
    "    import matplotlib\n",
    "    matplotlib.rc('text', usetex=True)\n",
    "    plt.rc('text', usetex=True)\n",
    "    plt.rc('font', family='serif')\n",
    "\n",
    "imagetable_path = osp.join(DATA_PATH, 'Tables', 'ImageTable.csv')\n",
    "image_df = pd.read_csv(imagetable_path, index_col=0)\n",
    "analysis_path = osp.join(DATA_PATH, 'Tables', 'Analysis.csv')\n",
    "dive_path = osp.join(DATA_PATH, 'Tables', 'Dives.csv')\n",
    "analysis_df = pd.read_csv(analysis_path, index_col=0, dtype={'Count': float})\n",
    "dive_df = pd.read_csv(dive_path, index_col=0, parse_dates=['Date'])\n",
    "data_df = pd.merge(analysis_df, dive_df, on='Dive Index', how='outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. a) Do predictions for all images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "8bwN2IhcgN0G",
    "outputId": "06676eec-0c7e-4733-887d-c47a7b211f9c"
   },
   "outputs": [],
   "source": [
    "prd_ct = []  # for storing the number of mussel pixels in each prediction\n",
    "\n",
    "# This cell takes some time because we're randomly reading large images from Google Drive\n",
    "# This loop runs at 1.5 seconds per image on my workstation.\n",
    "for i in tqdm_notebook(range(len(jpeg_files)), unit=' image'):\n",
    "\n",
    "    bgr_img = cv2.imread(osp.join(root_path, jpeg_files[i]))\n",
    "\n",
    "    img = cv2.cvtColor(bgr_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # pre-processing image consistent with PyTorch training transforms\n",
    "    nchw_tensor = img_to_nchw_tensor(img, device)\n",
    "\n",
    "    # saves memory in forward pass if no gradients required\n",
    "    with torch.no_grad():\n",
    "        pred = sig(net(nchw_tensor)['out'])\n",
    "\n",
    "    prd_ct.append(pred.round().sum().item() / np.prod(pred.shape[2:]))\n",
    "\n",
    "prd_ct_np = np.asarray(prd_ct)\n",
    "x_pred = prd_ct_np.copy()\n",
    "\n",
    "# Normalize inputs between 0 and 1\n",
    "x_pred = x_pred / x_pred.max()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. b) Load masks for all images\n",
    "\n",
    "This will be used as a baseline when predicting live coverage, count, biomass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_ct = []  # for storing the number of mussel pixels in each mask\n",
    "\n",
    "# This loop should run faster as model is not used, \n",
    "# it runs at 4.5 images per second on my workstation\n",
    "for i in tqdm_notebook(range(len(png_files)), unit=' image'):\n",
    "    \n",
    "    bgr_lab = cv2.imread(osp.join(root_path, png_files[i]))\n",
    "    _, cts = np.unique(bgr_lab, return_counts=True)\n",
    "\n",
    "    # check if mask contains both classes\n",
    "    if len(cts) > 1:\n",
    "        lab_ct.append(cts[1] / cts.sum())\n",
    "    # or only one class (background)\n",
    "    else:\n",
    "        lab_ct.append(0)\n",
    "\n",
    "lab_ct_np = np.asarray(lab_ct)        \n",
    "x_seg_label = lab_ct_np.copy()\n",
    "x_seg_label = x_seg_label / x_seg_label.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Denote segmentation masks `x_seg_label` and predictions `x_pred`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ioB337i5gN0J"
   },
   "outputs": [],
   "source": [
    "# 0 = biomass, 1 = count, 2 = live coverage\n",
    "lab_targets = np.zeros((len(jpeg_files), 3))\n",
    "\n",
    "for i in range(len(jpeg_files)):\n",
    "\n",
    "    # get unique identifier from jpeg filename which acts as a key into dataframe\n",
    "    root_fname = jpeg_files[i].split(\n",
    "        '/')[-1].split('.')[0].split('_image')[0].split('GLNI_')[1]\n",
    "\n",
    "    # get a globally unique ID corresponding to image\n",
    "    guid = image_df[image_df['Name'].str.contains(\n",
    "        root_fname)]['Analysis Index'].astype('int64')\n",
    "\n",
    "    # extract relevant row from data frame\n",
    "    row = data_df[data_df['Analysis Index'].values == np.unique(guid.values)]\n",
    "\n",
    "    # extract Biomass, Count, and Live Coverage from row\n",
    "    lab_targets[i, BIOMASS_IDX] = row['Biomass'].values\n",
    "    lab_targets[i, COUNT_IDX] = row['Count'].values\n",
    "    lab_targets[i, LIVE_COV_IDX] = row['Live Coverage'].values\n",
    "\n",
    "valid_biomass = np.invert(np.isnan(lab_targets[:, BIOMASS_IDX]))\n",
    "valid_count = np.invert(np.isnan(lab_targets[:, COUNT_IDX]))\n",
    "valid_live_cov = np.invert(np.isnan(lab_targets[:, LIVE_COV_IDX]))\n",
    "\n",
    "biomass = lab_targets[:, BIOMASS_IDX][valid_biomass]\n",
    "count = lab_targets[:, COUNT_IDX][valid_count]\n",
    "live_cov = lab_targets[:, LIVE_COV_IDX][valid_live_cov]\n",
    "\n",
    "# Normalize targets between 0 and 1\n",
    "biomass = biomass / biomass.max()\n",
    "count = count / count.max()\n",
    "live_cov = live_cov / live_cov.max()\n",
    "\n",
    "print('Got %d valid biomass measurements' % len(biomass))\n",
    "print('Got %d valid count measurements' % len(count))\n",
    "print('Got %d valid live coverage measurements' % len(live_cov))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Live Coverage, Biomass, and Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(12, 4.5), sharex=True, sharey=False)\n",
    "\n",
    "## live coverage ##\n",
    "ax[0].scatter(x_pred[valid_live_cov], live_cov, marker='o', s=40,\n",
    "              facecolors='none', edgecolors='k')\n",
    "ax[0].set_ylabel('Live Coverage', fontsize=fontsize)\n",
    "ax[0].set_title(r'$\\mathbf{R^2}$ = %.3f' % r2_score(\n",
    "    live_cov, linear_regression(x_pred[valid_live_cov], live_cov)), fontsize=fontsize)\n",
    "draw_lines(ax[0], x_pred[valid_live_cov], live_cov)\n",
    "\n",
    "## biomass ##\n",
    "ax[1].scatter(x_pred[valid_biomass], biomass, marker='o', s=40,\n",
    "              facecolors='none', edgecolors='k')\n",
    "ax[1].set_ylabel('Biomass', fontsize=fontsize)\n",
    "ax[1].set_xlabel('Fraction of Mussel Pixels \\n in Prediction',\n",
    "                 fontsize=fontsize)\n",
    "ax[1].set_title(r'$\\mathbf{R^2}$ = %.3f' % r2_score(biomass, linear_regression(\n",
    "    x_pred[valid_biomass], biomass)),  fontsize=fontsize + 1)\n",
    "draw_lines(ax[1], x_pred[valid_biomass], biomass)\n",
    "\n",
    "## count ##\n",
    "ax[2].scatter(x_pred[valid_count], count, marker='o',\n",
    "              s=40, facecolors='none', edgecolors='k')\n",
    "ax[2].set_ylabel('Count', fontsize=fontsize)\n",
    "ax[2].set_title(r'$\\mathbf{R^2}$ = %.3f' % r2_score(\n",
    "    x_pred[valid_count], linear_regression(x_pred[valid_count], count)), fontsize=fontsize + 1)\n",
    "draw_lines(ax[2], x_pred[valid_count], count)\n",
    "\n",
    "draw_sublabel(ax[0], r'\\textbf{a)}', fontsize, (0.85, 0.05))\n",
    "draw_sublabel(ax[1], r'\\textbf{b)}', fontsize, (0.85, 0.05))\n",
    "draw_sublabel(ax[2], r'\\textbf{c)}', fontsize, (0.85, 0.05))\n",
    "\n",
    "pretty_axis(ax[0], fontsize)\n",
    "pretty_axis(ax[1], fontsize)\n",
    "pretty_axis(ax[2], fontsize)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# uncomment to save figure\n",
    "#fname = 'val_v101_livecov_biomass_and_count_from_predictions_abc'\n",
    "#fig.savefig(fname + '.png')\n",
    "#fig.savefig(fname + '.eps', format='eps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Only continue if you wish to compare against segmentation masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Live Coverage\n",
    "\n",
    "From segmentation masks `x_seg_label` and predictions `x_pred`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(8, 4), sharex=True, sharey=True)\n",
    "\n",
    "ax[0].scatter(x_seg_label[valid_live_cov], live_cov, marker='o',\n",
    "              s=40, facecolors='none', edgecolors='k')\n",
    "ax[1].scatter(x_pred[valid_live_cov], live_cov, marker='o',\n",
    "              s=40, facecolors='none', edgecolors='k')\n",
    "\n",
    "ax[0].set_ylabel('Live Coverage', fontsize=fontsize)\n",
    "ax[0].set_xlabel(\n",
    "    'Fraction of Mussel Pixels \\n in Segmentation Mask', fontsize=fontsize)\n",
    "ax[1].set_xlabel('Fraction of Mussel Pixels \\n in Prediction',\n",
    "                 fontsize=fontsize)\n",
    "\n",
    "draw_lines(ax[0], x_seg_label[valid_live_cov], live_cov)\n",
    "draw_lines(ax[1], x_pred[valid_live_cov], live_cov)\n",
    "\n",
    "ax[0].set_title(r'$\\mathbf{R^2}$ = %.3f' % r2_score(live_cov, linear_regression(\n",
    "    x_seg_label[valid_live_cov], live_cov)),  fontsize=fontsize)\n",
    "ax[1].set_title(r'$\\mathbf{R^2}$ = %.3f' % r2_score(live_cov, linear_regression(\n",
    "    x_pred[valid_live_cov], live_cov)),  fontsize=fontsize)\n",
    "\n",
    "pretty_axis(ax[0], fontsize)\n",
    "pretty_axis(ax[1], fontsize)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Biomass\n",
    "\n",
    "From segmentation masks `x_seg_label` and predictions `x_pred`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "colab_type": "code",
    "id": "IA-2nitYgN0N",
    "outputId": "1bbca420-004a-44b6-c41c-78ffccfc46f1"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(8, 4), sharex=True, sharey=True)\n",
    "\n",
    "ax[0].scatter(x_seg_label[valid_biomass], biomass, marker='o',\n",
    "              s=40, facecolors='none', edgecolors='k')\n",
    "ax[1].scatter(x_pred[valid_biomass], biomass, marker='o',\n",
    "              s=40, facecolors='none', edgecolors='k')\n",
    "ax[0].set_ylabel('Biomass', fontsize=fontsize)\n",
    "ax[0].set_xlabel(\n",
    "    'Fraction of Mussel Pixels \\n in Segmentation Mask', fontsize=fontsize)\n",
    "ax[1].set_xlabel('Fraction of Mussel Pixels \\n in Prediction',\n",
    "                 fontsize=fontsize)\n",
    "\n",
    "draw_lines(ax[0], x_seg_label[valid_biomass], biomass)\n",
    "draw_lines(ax[1], x_pred[valid_biomass], biomass)\n",
    "\n",
    "ax[0].set_title(r'$\\mathbf{R^2}$ = %.3f' % r2_score(biomass, linear_regression(\n",
    "    x_seg_label[valid_biomass], biomass)),  fontsize=fontsize)\n",
    "ax[1].set_title(r'$\\mathbf{R^2}$ = %.3f' % r2_score(\n",
    "    biomass, linear_regression(x_pred[valid_biomass], biomass)),  fontsize=fontsize)\n",
    "\n",
    "pretty_axis(ax[0], fontsize)\n",
    "pretty_axis(ax[1], fontsize)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Count\n",
    "\n",
    "From segmentation masks `x_seg_label` and predictions `x_pred`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(8, 4), sharex=True, sharey=True)\n",
    "\n",
    "ax[0].scatter(x_seg_label[valid_count], count, marker='o',\n",
    "              s=40, facecolors='none', edgecolors='k')\n",
    "ax[1].scatter(x_pred[valid_count], count, marker='o',\n",
    "              s=40, facecolors='none', edgecolors='k')\n",
    "ax[0].set_ylabel('Count', fontsize=fontsize)\n",
    "ax[0].set_xlabel(\n",
    "    'Fraction of Mussel Pixels \\n in Segmentation Mask', fontsize=fontsize)\n",
    "ax[1].set_xlabel('Fraction of Mussel Pixels \\n in Prediction',\n",
    "                 fontsize=fontsize)\n",
    "\n",
    "draw_lines(ax[0], x_seg_label[valid_count], count)\n",
    "draw_lines(ax[1], x_pred[valid_count], count)\n",
    "\n",
    "ax[0].set_title(r'$\\mathbf{R^2}$ = %.3f' % r2_score(\n",
    "    count, linear_regression(x_seg_label[valid_count], count)),  fontsize=fontsize)\n",
    "ax[1].set_title(r'$\\mathbf{R^2}$ = %.3f' % r2_score(\n",
    "    count, linear_regression(x_pred[valid_count], count)),  fontsize=fontsize)\n",
    "\n",
    "pretty_axis(ax[0], fontsize)\n",
    "pretty_axis(ax[1], fontsize)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "task_3_evaluate_checkpoint_in_colab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
