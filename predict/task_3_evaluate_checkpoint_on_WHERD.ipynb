{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0UXd9CuzgNx2"
   },
   "source": [
    "# Evaluate a Pre-Trained Segmentation Model on WHERD\n",
    "\n",
    "Demonstrates image pre-processing, prediction and validation statistics. But first, some preliminaries...\n",
    "\n",
    "__Note:__ To maintain a high priority Colab user status such that sufficient GPU resources are available in the future, ensure to free the runtime when finished running this notebook. This can be done using 'Runtime > Manage Sessions' and click 'Terminate'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if notebook is running in Colab or local workstation\n",
    "import sys\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    !ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
    "    !pip install gputil\n",
    "    !pip install psutil\n",
    "    !pip install humanize\n",
    "\n",
    "import psutil\n",
    "import humanize\n",
    "import os\n",
    "import GPUtil as GPU\n",
    "GPUs = GPU.getGPUs()\n",
    "\n",
    "try:\n",
    "    # XXX: only one GPU on Colab and isn’t guaranteed\n",
    "    gpu = GPUs[1]\n",
    "    def printm():\n",
    "        process = psutil.Process(os.getpid())\n",
    "        print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
    "        print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
    "    printm() \n",
    "\n",
    "    # Check if GPU capacity is sufficient to proceed\n",
    "    if gpu.memoryFree < 10000:\n",
    "        print(\"\\nInsufficient memory! Some cells may fail. Please try restarting the runtime using 'Runtime → Restart Runtime...' from the menu bar. If that doesn't work, terminate this session and try again later.\")\n",
    "    else:\n",
    "        print('\\nGPU memory is sufficient to proceeed.')\n",
    "except:\n",
    "    print('Select the Runtime → \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
    "    print('and then re-execute this cell.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "4pzGY41DgyyQ",
    "outputId": "6bebf55b-f9f5-4a59-abdd-4c101878f4d1"
   },
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    DATA_PATH = r'/content/drive/My Drive/Data'\n",
    "    \n",
    "    # cd into git repo so python can find utils\n",
    "    %cd '/content/drive/My Drive/cciw-zebra-mussel/predict'\n",
    "\n",
    "    sys.path.append('/content/drive/My Drive')\n",
    "    \n",
    "    # clone repo, install packages not installed by default\n",
    "    !pip install pydensecrf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bneyBxcYgNx7"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "\n",
    "import glob\n",
    "\n",
    "# for manually reading high resolution images\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "\n",
    "# for comparing predictions to lab analysis data frames\n",
    "import pandas as pd\n",
    "\n",
    "# for plotting\n",
    "import matplotlib\n",
    "# enable LaTeX style fonts\n",
    "matplotlib.rc('text', usetex=True)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rc('text', usetex=True)\n",
    "plt.rc('font', family='serif')\n",
    "\n",
    "# pytorch core library\n",
    "import torch\n",
    "# pytorch neural network functions\n",
    "from torch import nn\n",
    "# pytorch dataloader\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# for post-processing model predictions by conditional random field \n",
    "import pydensecrf.densecrf as dcrf\n",
    "import pydensecrf.utils as utils\n",
    "\n",
    "from tqdm import tqdm  # progress bar\n",
    "\n",
    "# evaluation metrics\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import jaccard_score as jsc\n",
    "\n",
    "# local imports (files provided by this repo)\n",
    "import transforms as T\n",
    "\n",
    "# various helper functions, metrics that can be evaluated on the GPU\n",
    "from task_3_utils import evaluate, evaluate_loss, eval_binary_iou, pretty_image\n",
    "\n",
    "# Custom dataloader for rapidly loading images from a single LMDB file\n",
    "from folder2lmdb import VOCSegmentationLMDB\n",
    "\n",
    "# for finding laser beams\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Confim that this cell prints \"Found GPU, cuda\". If not, select \"GPU\" as \n",
    "\"Hardware Accelerator\" under the \"Runtime\" tab of the main menu.\n",
    "\"\"\"\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print('Found GPU,', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n4P0mldogNyQ"
   },
   "source": [
    "## 1. Load a pre-trained model checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['DATA_PATH'] = '/scratch/gallowaa/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 3\n",
    "\n",
    "if IN_COLAB:\n",
    "    root = osp.join(\n",
    "        DATA_PATH, 'Checkpoints/deeplabv3_resnet50_lr1e-01_wd5e-04_bs40_ep80_seed1')\n",
    "    \n",
    "else:\n",
    "    root = osp.join(\n",
    "        os.environ['DATA_PATH'], 'cciw/logs/cmp-dataset/train_v120/deeplabv3_resnet50/lr1e-01/wd5e-04/bs40/ep80/seed%d/checkpoint' % SEED)\n",
    "    #root = osp.join(\n",
    "    #    os.environ['DATA_PATH'], 'cciw/logs/cmp-dataset/deeplab/trainval_v120/deeplabv3_resnet50/lr1e-01/wd5e-04/bs40/ep80/seed1/checkpoint/')\n",
    "\n",
    "ckpt_file = 'deeplabv3_resnet50_lr1e-01_wd5e-04_bs40_ep80_seed%d_epoch79.ckpt' % SEED\n",
    "#ckpt_file = 'deeplabv3_resnet50_lr1e-01_wd5e-04_bs40_ep80_seed%d_epoch40.ckpt' % SEED\n",
    "\n",
    "model_to_load = osp.join(root, ckpt_file)\n",
    "                         \n",
    "print('Loading', model_to_load)                         \n",
    "         \n",
    "checkpoint = torch.load(model_to_load)\n",
    "                        \n",
    "train_loss = checkpoint['trn_loss']\n",
    "val_loss = checkpoint['val_loss']\n",
    "print('==> Resuming from checkpoint..')\n",
    "net = checkpoint['net']\n",
    "last_epoch = checkpoint['epoch']\n",
    "torch.set_rng_state(checkpoint['rng_state'])\n",
    "\n",
    "# later appended to figure filenames\n",
    "model_stem = ckpt_file.split('.')[0]\n",
    "\n",
    "print('Loaded model %s trained to epoch ' % model_stem, last_epoch)\n",
    "print(\n",
    "    'Cross-entropy loss {:.4f} for train set, {:.4f} for validation set'.format(train_loss, val_loss))\n",
    "\n",
    "sig = nn.Sigmoid()  # initializes a sigmoid function\n",
    "\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from fcn import FCN8s\n",
    "net = FCN8s(n_class=1).to(device)\n",
    "\n",
    "from apex import amp\n",
    "net = amp.initialize(net, opt_level='O3')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = osp.join(os.environ['DATA_PATH'], 'cciw/logs/cmp-dataset/train_v120/')\n",
    "print(root)\n",
    "\n",
    "#files = glob.glob(root + '*/*/*/*/*/*/checkpoint/*epoch79.ckpt')\n",
    "files = glob.glob(root + 'deeplab*/*/*/*/*/*/checkpoint/*epoch79.ckpt')\n",
    "\n",
    "print(len(files))\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "n89QOGLxgNyS",
    "outputId": "904c901b-f3e2-4a84-afd8-27eaad8aadf7"
   },
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    root = osp.join(DATA_PATH, 'Checkpoints/fcn8slim_lr1e-03_wd5e-04_bs32_ep50_seed1')\n",
    "else:\n",
    "    pass\n",
    "    #root = '/scratch/gallowaa/cciw/logs/v1.0.1-debug/fcn8s/lr1e-03/wd5e-04/bs25/ep80/seed4/checkpoint' # b\n",
    "    #root = '/scratch/gallowaa/cciw/logs/v111/trainval/fcn8s/lr1e-03/wd5e-04/bs40/ep80/seed2/checkpoint/' # d\n",
    "\n",
    "#ckpt_file = 'fcn8s_lr1e-03_wd5e-04_bs25_ep80_seed4_epoch70.ckpt' # b\n",
    "#ckpt_file = 'fcn8s_lr1e-03_wd5e-04_bs40_ep80_seed2amp_epoch79.pt' # d\n",
    "\n",
    "\"\"\"Feel free to try these other checkpoints later after running epoch40 to get a \n",
    "feel for how the evaluation metrics change when model isn't trained as long.\"\"\"\n",
    "\n",
    "#checkpoint = torch.load(osp.join(root, ckpt_file))\n",
    "f = 2\n",
    "checkpoint = torch.load(files[f])\n",
    "train_loss = checkpoint['trn_loss']\n",
    "val_loss = checkpoint['val_loss']\n",
    "print('==> Resuming from checkpoint..')\n",
    "\n",
    "net = checkpoint['net']\n",
    "'''\n",
    "# AMP\n",
    "net.load_state_dict(checkpoint['net'])\n",
    "amp.load_state_dict(checkpoint['amp'])\n",
    "'''\n",
    "last_epoch = checkpoint['epoch'] + 1\n",
    "torch.set_rng_state(checkpoint['rng_state'])\n",
    "\n",
    "# later appended to figure filenames\n",
    "#model_stem = ckpt_file.split('.')[0]\n",
    "model_stem = files[f].split('/')[-1].split('.')[0]\n",
    "\n",
    "print('Loaded model %s trained to epoch ' % model_stem, last_epoch)\n",
    "print('Cross-entropy loss {:.4f} for train set, {:.4f} for validation set'.format(train_loss, val_loss))\n",
    "\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#net = amp.initialize(net, opt_level='O3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UnzXpijsgNzC"
   },
   "outputs": [],
   "source": [
    "sig = nn.Sigmoid()  # initializes a sigmoid function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ne_OPnTPgNzh"
   },
   "source": [
    "## 7. i) Visualize Predictions on Whole Images\n",
    "\n",
    "Here we manually load and preprocess the original images and png masks using OpenCV.\n",
    "\n",
    "`root_path` -- will also be used in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "rouYvCy3gNzj",
    "outputId": "0cd6d473-73c3-4f2e-97ca-df03fbb55f7a"
   },
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    root_path = osp.join(DATA_PATH, 'ADIG_Labelled_Dataset/Test/Lab/')\n",
    "else:\n",
    "    root_path = '/scratch/ssd/gallowaa/cciw/dataset_raw/Test/WHERD/all/'\n",
    "    \n",
    "jpeg_files = glob.glob(root_path + '*.jpg')\n",
    "png_files = glob.glob(root_path + '*_final.png')\n",
    "\n",
    "jpeg_files.sort()\n",
    "png_files.sort()\n",
    "\n",
    "# Both should equal 16 for all WHERD dataset\n",
    "print(len(jpeg_files)) \n",
    "print(len(png_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jpeg_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Set to True to save the model predictions in PNG format, \n",
    "otherwise proceed to predict biomass without saving images\"\"\"\n",
    "PLOT = False\n",
    "SAVE_PREDICTIONS = False\n",
    "\n",
    "if SAVE_PREDICTIONS:\n",
    "    prediction_path = ''\n",
    "    for t in model_to_load.split('/')[:-1]:\n",
    "    #for t in files[f].split('/')[:-1]:\n",
    "        prediction_path += t + '/'\n",
    "\n",
    "    prediction_path = osp.join(prediction_path, 'WHERD')\n",
    "\n",
    "    if not osp.exists(prediction_path):\n",
    "        os.mkdir(prediction_path)\n",
    "\n",
    "    print(prediction_path)\n",
    "    \n",
    "    # src is the training dataset, tgt is the testing dataset\n",
    "    src = 'train_v120'\n",
    "    tgt = 'WHERD'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fontsize = 16\n",
    "\n",
    "left = 0.02  # the left side of the subplots of the figure\n",
    "right = 0.98   # the right side of the subplots of the figure\n",
    "bottom = 0.05  # the bottom of the subplots of the figure\n",
    "top = 0.95     # the top of the subplots of the figure\n",
    "wspace = 0.15  # the amount of width reserved for space between subplots,\n",
    "# expressed as a fraction of the average axis width\n",
    "hspace = 0.1  # the amount of height reserved for space between subplots,\n",
    "# expressed as a fraction of the average axis height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from task_3_utils import img_to_nchw_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_scales = np.array([157,  # 1353\n",
    "                          65,  # 1355\n",
    "                          33,  # FS1355\n",
    "                          40,  # FSHP131642\n",
    "                          37,  # FSHP131645\n",
    "                          100,  # FSHP131649 (55-115)\n",
    "                          23,  # FSLEE06022\n",
    "                          80,  # FSLEE06037 (40, 80, 110)\n",
    "                          125,  # HP115410\n",
    "                          100,  # HP131641\n",
    "                          150,  # LEE06021\n",
    "                          32,  # LEE06031 # (30-32, 60, 130)\n",
    "                          100,  # LEE06032\n",
    "                          100,  # LEE06033\n",
    "                          100,  # LEE06070\n",
    "                          60])  # LEE06075"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(search_scales)):\n",
    "    print(search_scales[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# line search for optimal scale\n",
    "i = 6\n",
    "scales = np.arange(20, 150, 5)\n",
    "#scales = np.arange(30, 65, 1)\n",
    "image_stem = jpeg_files[i].split('/')[-1].split('.')[0]\n",
    "bgr_lab = cv2.imread(osp.join(root_path, png_files[i]))\n",
    "bgr_img = cv2.imread(osp.join(root_path, jpeg_files[i]))\n",
    "labc = cv2.cvtColor(bgr_lab, cv2.COLOR_BGR2RGB)\n",
    "imgc = cv2.cvtColor(bgr_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "print(image_stem.split('_')[1])\n",
    "for scale_percent in scales:\n",
    "    \n",
    "    img = resize(imgc, scale_percent)\n",
    "    lab = resize(labc, scale_percent)\n",
    "    \n",
    "    nchw_tensor = img_to_nchw_tensor(img, device)\n",
    "    with torch.no_grad():\n",
    "        pred = sig(net(nchw_tensor)['out'])\n",
    "    pred_np = pred.detach().cpu().numpy()\n",
    "    mask = np.zeros((lab.shape[0], lab.shape[1]), dtype='float32')\n",
    "    mask[lab[:, :, 0] == 128] = 1\n",
    "    pred_np = pred_np.squeeze()\n",
    "    targets = torch.LongTensor(mask)\n",
    "    targets = targets.to(device)\n",
    "    iou = eval_binary_iou(pred, targets).item()\n",
    "    print(scale_percent, iou)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find laser beams in image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# meta-parameters for algorithm to find two red laser dots\n",
    "# red_threshold is out of 255, match_threshold out of 1.0 for template matching\n",
    "# red_threshold, match_threshold = 200, 0.2 # 10 correct\n",
    "intensity_threshold, match_threshold = 150, 0.3  # 10\n",
    "\n",
    "\"\"\"C - Constant subtracted from the mean or weighted mean (see the details below). \n",
    "Normally, it is positive but may be zero or negative as well.\"\"\"\n",
    "C = -15\n",
    "\n",
    "\"\"\"Block size - Size of a pixel neighborhood that is used to calculate a threshold \n",
    "value for the pixel: 3, 5, 7, and so on.\"\"\"\n",
    "block_size = 201\n",
    "\n",
    "# noise removing kernel\n",
    "kernel = np.ones((5, 5), np.uint8)\n",
    "dilate_kernel = np.ones((6, 6), np.uint8)\n",
    "\n",
    "j = 0\n",
    "laser_distances = []\n",
    "\n",
    "for i in range(len(jpeg_files)):\n",
    "    f = jpeg_files[i]\n",
    "    #if f.split('/')[-1] in include_list:\n",
    "    im = cv.imread(f)\n",
    "    print(im.shape)\n",
    "    #outfile = os.path.join(save_path, f.split('/')[-1])\n",
    "\n",
    "    im = cv.imread(f)  # im will be in BGR format\n",
    "    im_thresh = im.copy()\n",
    "\n",
    "    im_ycrbc = cv.cvtColor(im, cv.COLOR_BGR2YCrCb)\n",
    "\n",
    "    Y  = im_ycrbc[:, :, 0] # Separate pixel intensity Y from \n",
    "    Cr = im_ycrbc[:, :, 1] # red-difference chroma \n",
    "    Cb = im_ycrbc[:, :, 2] # blue-difference components\n",
    "\n",
    "    im_thresh[Cr < intensity_threshold] = 0\n",
    "\n",
    "    cts, bin_edges = np.histogram(im_ycrbc[:, :, 2])\n",
    "    red_range = bin_edges.max() - bin_edges.min()\n",
    "\n",
    "    Cr_th = cv.adaptiveThreshold(\n",
    "        Cr, 255, cv.ADAPTIVE_THRESH_GAUSSIAN_C, cv.THRESH_BINARY, 201, C)\n",
    "\n",
    "    # morphology and connected components\n",
    "    opening = cv.morphologyEx(Cr_th, cv.MORPH_OPEN, kernel, iterations=2)\n",
    "    sure_bg = cv.dilate(opening, dilate_kernel, iterations=3)\n",
    "\n",
    "    # Marker labelling\n",
    "    ret, markers = cv.connectedComponents(sure_bg)\n",
    "\n",
    "    # get the cluster IDs and number of pixels in each\n",
    "    ids, cts = np.unique(markers, return_counts=True)\n",
    "\n",
    "    # only keep the largest two clusters by pixel count\n",
    "    largest_blobs = ids[1:][np.argsort(cts[1:])][-2:]\n",
    "\n",
    "    if len(largest_blobs) == 2:\n",
    "        mask = markers == largest_blobs[0]\n",
    "        mask |= markers == largest_blobs[1]\n",
    "        markers[np.invert(mask)] = 0\n",
    "\n",
    "    # we can now encode clusters with the same value as\n",
    "    # they should be physically separated\n",
    "    markers[markers > 0] = 1\n",
    "\n",
    "    # get numpy array of coordinates of non-zero elements\n",
    "    coords = cv.findNonZero(markers.astype('uint8'))\n",
    "\n",
    "    if coords is not None:\n",
    "        kmeans = KMeans(n_clusters=2, random_state=0).fit(coords.squeeze())\n",
    "        # draw the two clusters\n",
    "        for pt in kmeans.cluster_centers_:\n",
    "            _ = cv.circle(im, (int(pt[0]), int(pt[1])), 50, (0, 255, 0), 2)\n",
    "\n",
    "        # compute the distance in pixels between the two laser beams\n",
    "        d = np.linalg.norm(\n",
    "            kmeans.cluster_centers_[0] - kmeans.cluster_centers_[1])\n",
    "    else:\n",
    "        d = -1  # do not change scale if -1\n",
    "    \n",
    "    laser_distances.append(d)\n",
    "    '''\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(14, 6))\n",
    "    title_str = '%d Ratio %.3f - dots are dist %d of %d px' % (j, d/im.shape[1], d, im.shape[1])\n",
    "    ax.imshow(im)\n",
    "    ax.set_title(title_str)\n",
    "    plt.show()\n",
    "    '''\n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jpeg_file_resolutions = []\n",
    "for i in range(len(jpeg_files)):\n",
    "    jpeg_file_resolutions.append(np.prod(cv.imread(jpeg_files[i]).shape[:2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laser_distances = np.round(np.asarray(laser_distances), 3)\n",
    "#mask = laser_distances > 0\n",
    "#lasers = laser_distances[mask]\n",
    "lasers = laser_distances / laser_distances.max()\n",
    "lasers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jpeg_file_resolutions = np.asarray(jpeg_file_resolutions)\n",
    "jpeg_file_resolutions = jpeg_file_resolutions / float(jpeg_file_resolutions.max())\n",
    "res = jpeg_file_resolutions.copy()#[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide by the resolution so that less resolution = larger distance\n",
    "d = 1 / (lasers / res)\n",
    "d = d / d.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(d, 2)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.round(d, 2)*100)\n",
    "#plt.plot(search_scales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laser_scales = (d * 100).astype(int)\n",
    "laser_scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(laser_scales)):\n",
    "    print(laser_scales[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize(image, pct):\n",
    "    width = int(image.shape[1] * pct / 100)\n",
    "    height = int(image.shape[0] * pct / 100)\n",
    "    return cv2.resize(image, (width, height))  # resize image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "cxaxZTfigNzs",
    "outputId": "394b6282-16e5-4596-dd56-4272301189a4",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "iou_list = []\n",
    "#scale_percent = 100\n",
    "\n",
    "for i in range(len(jpeg_files)):\n",
    "    image_stem = jpeg_files[i].split('/')[-1].split('.')[0]\n",
    "    bgr_lab = cv2.imread(osp.join(root_path, png_files[i]))\n",
    "    bgr_img = cv2.imread(osp.join(root_path, jpeg_files[i]))\n",
    "    lab = cv2.cvtColor(bgr_lab, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.cvtColor(bgr_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    scale_percent = laser_scales[i] if laser_scales[i] > 0 else 100\n",
    "    #scale_percent = search_scales[i]\n",
    "    img = resize(img, scale_percent)\n",
    "    lab = resize(lab, scale_percent)\n",
    "\n",
    "    nchw_tensor = img_to_nchw_tensor(img, device)\n",
    "    with torch.no_grad():\n",
    "        pred = sig(net(nchw_tensor)['out'])\n",
    "    pred_np = pred.detach().cpu().numpy().squeeze()\n",
    "    mask = np.zeros((lab.shape[0], lab.shape[1]), dtype='float32')\n",
    "    mask[lab[:, :, 0] == 128] = 1\n",
    "    \n",
    "    targets = torch.LongTensor(mask)\n",
    "    targets = targets.to(device)\n",
    "    iou = eval_binary_iou(pred, targets).item()\n",
    "    '''\n",
    "    p_one_hot, t_one_hot = mask_and_preds_to_1hot(pred_np, mask)\n",
    "\n",
    "    iou = jsc(p_one_hot.reshape(1, -1),\n",
    "              t_one_hot.reshape(1, -1), average='samples')\n",
    "    '''\n",
    "    iou_list.append(iou)\n",
    "    print('%d, %.4f' % (i, iou))\n",
    "\n",
    "    if PLOT:\n",
    "        # plt.close('all')\n",
    "        fig, axes = plt.subplots(1, 1, figsize=(19.20, 10.80))\n",
    "        p = (pred_np * 255).astype('uint8')\n",
    "        src2 = np.zeros((p.shape[0], p.shape[1], 3), np.uint8)\n",
    "        src2[:, :, 2] = p\n",
    "        dst = cv2.addWeighted(img, 0.5, src2, 0.5, 0)\n",
    "        axes.imshow(dst)\n",
    "        axes.axis('off')\n",
    "        plt.tight_layout()\n",
    "    if SAVE_PREDICTIONS:\n",
    "        filename = src + '__' + image_stem + '__' + model_stem + \\\n",
    "            '_scale%d_iou_%.4f' % (scale_percent, iou)\n",
    "        out_file = osp.join(prediction_path, filename)\n",
    "        fig.savefig(out_file + '.jpg', format='jpeg')\n",
    "        \n",
    "print(\"Average \", np.round(np.asarray(iou_list).mean(), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iou = jsc(p_one_hot.reshape(1, -1),\n",
    "          t_one_hot.reshape(1, -1), average='samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0 0.3349668479370011\n",
    "1 0.2730633762824413\n",
    "2 0.08701870141603765\n",
    "3 0.6963940582031034\n",
    "4 0.7138648992434893\n",
    "5 0.2700486397208539\n",
    "6 0.6887657497196386\n",
    "7 0.943557536979472\n",
    "8 0.42190770560009877\n",
    "9 0.8012735566943816\n",
    "10 0.9509239510349701\n",
    "11 0.9770874858352112\n",
    "12 0.8296693915528675\n",
    "13 0.44408103316978087\n",
    "14 0.9843869248270862\n",
    "15 0.8617212302299858\n",
    "mean 0.6424206930279013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_and_preds_to_1hot(p, y):\n",
    "    \n",
    "    # labels y to one hot encoding\n",
    "    y_1hot = np.zeros((2, y.shape[0], y.shape[1]))\n",
    "    y_1hot[1, :, :][y == 1] = 1\n",
    "    y_1hot[0, :, :][y == 0] = 1\n",
    "\n",
    "    # predictions p to one hot encoding\n",
    "    p_1hot = np.zeros((2, p.shape[0], p.shape[1]))\n",
    "    p_1hot[1, :, :][p.squeeze().round() == 1] = 1\n",
    "    p_1hot[0, :, :][p.squeeze().round() == 0] = 1\n",
    "    \n",
    "    return p_1hot, y_1hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOM: Figure 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 4\n",
    "\n",
    "image_stem = jpeg_files[i].split('/')[-1].split('.')[0]\n",
    "bgr_lab = cv2.imread(osp.join(root_path, png_files[i]))\n",
    "bgr_img = cv2.imread(osp.join(root_path, jpeg_files[i]))\n",
    "lab = cv2.cvtColor(bgr_lab, cv2.COLOR_BGR2RGB)\n",
    "img = cv2.cvtColor(bgr_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "\n",
    "scale_percent = 38\n",
    "#for scale_percent in scales:\n",
    "width = int(imgc.shape[1] * scale_percent / 100)\n",
    "height = int(imgc.shape[0] * scale_percent / 100)\n",
    "img = cv2.resize(imgc, (width, height)) # resize image\n",
    "lab = cv2.resize(labc, (width, height)) # resize image\n",
    "\n",
    "nchw_tensor = img_to_nchw_tensor(img, device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    pred = sig(net(nchw_tensor)['out'])\n",
    "\n",
    "pred_np = pred.detach().cpu().numpy().squeeze()\n",
    "\n",
    "mask = np.zeros((lab.shape[0], lab.shape[1]), dtype='float32')\n",
    "mask[lab[:, :, 0] == 128] = 1\n",
    "\n",
    "'''\n",
    "targets = torch.LongTensor(mask)\n",
    "targets = targets.to(device)\n",
    "#print(targets.shape)\n",
    "iou = eval_binary_iou(pred, targets).item()\n",
    "'''\n",
    "\n",
    "p_one_hot, t_one_hot = mask_and_preds_to_1hot(pred_np, mask)\n",
    "\n",
    "iou = jsc(p_one_hot.reshape(1, -1),\n",
    "          t_one_hot.reshape(1, -1), average='samples')\n",
    "\n",
    "#iou_list.append(iou)\n",
    "print(i, iou)\n",
    "\n",
    "#if PLOT:\n",
    "\n",
    "plt.close('all')\n",
    "fig, axes = plt.subplots(1, 1, figsize=(19.20, 10.80))\n",
    "p = (pred_np * 255).astype('uint8')\n",
    "src2 = np.zeros((p.shape[0], p.shape[1], 3), np.uint8)\n",
    "src2[:, :, 2] = p\n",
    "dst = cv2.addWeighted(img, 0.75, src2, 0.5, 0)\n",
    "axes.imshow(dst)\n",
    "axes.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "'''\n",
    "if SAVE_PREDICTIONS:\n",
    "    filename = src + '__' + image_stem + '__' + model_stem + '_scale%d_iou_%.4f' % (scale_percent, iou)\n",
    "    out_file = osp.join(prediction_path, filename)\n",
    "    fig.savefig(out_file + '.jpg', format='jpeg')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "fig, axes = plt.subplots(1, 1, figsize=(12.80, 7.20))\n",
    "p = (pred_np * 255).astype('uint8')\n",
    "src2 = np.zeros((p.shape[0], p.shape[1], 3), np.uint8)\n",
    "src2[:, :, 2] = p\n",
    "dst = cv2.addWeighted(img, 0.75, src2, 0.75, 0.25)\n",
    "axes.imshow(dst)\n",
    "axes.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'fig5-' + src + '-' + tgt + '__' + image_stem + '__' + model_stem\n",
    "out_file = osp.join(prediction_path, filename)\n",
    "fig.savefig(out_file + '_scale%d.jpg' % scale_percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "task_3_evaluate_checkpoint_in_colab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
