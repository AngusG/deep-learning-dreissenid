{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0UXd9CuzgNx2"
   },
   "source": [
    "# Evaluate a Pre-Trained Segmentation Model on WHERD\n",
    "\n",
    "Demonstrates image pre-processing, prediction and validation statistics. But first, some preliminaries...\n",
    "\n",
    "__Note:__ To maintain a high priority Colab user status such that sufficient GPU resources are available in the future, ensure to free the runtime when finished running this notebook. This can be done using 'Runtime > Manage Sessions' and click 'Terminate'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gen RAM Free: 66.2 GB  | Proc size: 54.1 MB\n",
      "GPU RAM Free: 11176MB | Used: 2MB | Util   0% | Total 11178MB\n",
      "\n",
      "GPU memory is sufficient to proceeed.\n"
     ]
    }
   ],
   "source": [
    "# Check if notebook is running in Colab or local workstation\n",
    "import sys\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    !ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
    "    !pip install gputil\n",
    "    !pip install psutil\n",
    "    !pip install humanize\n",
    "\n",
    "import psutil\n",
    "import humanize\n",
    "import os\n",
    "import GPUtil as GPU\n",
    "GPUs = GPU.getGPUs()\n",
    "\n",
    "try:\n",
    "    # XXX: only one GPU on Colab and isn’t guaranteed\n",
    "    gpu = GPUs[1]\n",
    "    def printm():\n",
    "        process = psutil.Process(os.getpid())\n",
    "        print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
    "        print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
    "    printm() \n",
    "\n",
    "    # Check if GPU capacity is sufficient to proceed\n",
    "    if gpu.memoryFree < 10000:\n",
    "        print(\"\\nInsufficient memory! Some cells may fail. Please try restarting the runtime using 'Runtime → Restart Runtime...' from the menu bar. If that doesn't work, terminate this session and try again later.\")\n",
    "    else:\n",
    "        print('\\nGPU memory is sufficient to proceeed.')\n",
    "except:\n",
    "    print('Select the Runtime → \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
    "    print('and then re-execute this cell.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "4pzGY41DgyyQ",
    "outputId": "6bebf55b-f9f5-4a59-abdd-4c101878f4d1"
   },
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    DATA_PATH = r'/content/drive/My Drive/Data'\n",
    "    \n",
    "    # cd into git repo so python can find utils\n",
    "    %cd '/content/drive/My Drive/cciw-zebra-mussel/predict'\n",
    "\n",
    "    sys.path.append('/content/drive/My Drive')\n",
    "    \n",
    "    # clone repo, install packages not installed by default\n",
    "    !pip install pydensecrf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bneyBxcYgNx7"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "\n",
    "import glob\n",
    "\n",
    "# for manually reading high resolution images\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "\n",
    "# for comparing predictions to lab analysis data frames\n",
    "import pandas as pd\n",
    "\n",
    "# for plotting\n",
    "import matplotlib\n",
    "# enable LaTeX style fonts\n",
    "matplotlib.rc('text', usetex=True)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rc('text', usetex=True)\n",
    "plt.rc('font', family='serif')\n",
    "\n",
    "# pytorch core library\n",
    "import torch\n",
    "# pytorch neural network functions\n",
    "from torch import nn\n",
    "# pytorch dataloader\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# for post-processing model predictions by conditional random field \n",
    "import pydensecrf.densecrf as dcrf\n",
    "import pydensecrf.utils as utils\n",
    "\n",
    "from tqdm import tqdm  # progress bar\n",
    "\n",
    "# evaluation metrics\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import jaccard_score as jsc\n",
    "\n",
    "# local imports (files provided by this repo)\n",
    "import transforms as T\n",
    "\n",
    "# various helper functions, metrics that can be evaluated on the GPU\n",
    "from task_3_utils import evaluate, evaluate_loss, eval_binary_iou, pretty_image\n",
    "\n",
    "# Custom dataloader for rapidly loading images from a single LMDB file\n",
    "from folder2lmdb import VOCSegmentationLMDB\n",
    "\n",
    "# for finding laser beams\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU, cuda\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Confim that this cell prints \"Found GPU, cuda\". If not, select \"GPU\" as \n",
    "\"Hardware Accelerator\" under the \"Runtime\" tab of the main menu.\n",
    "\"\"\"\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print('Found GPU,', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n4P0mldogNyQ"
   },
   "source": [
    "## 1. Load a pre-trained model checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['DATA_PATH'] = '/scratch/gallowaa/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading /scratch/gallowaa/cciw/logs/cmp-dataset/train_v120/deeplabv3_resnet50/lr1e-01/wd5e-04/bs40/ep80/seed3/checkpoint/deeplabv3_resnet50_lr1e-01_wd5e-04_bs40_ep80_seed3_epoch79.ckpt\n",
      "==> Resuming from checkpoint..\n",
      "Loaded model deeplabv3_resnet50_lr1e-01_wd5e-04_bs40_ep80_seed3_epoch79 trained to epoch  79\n",
      "Cross-entropy loss 0.1598 for train set, 0.9088 for validation set\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeepLabV3(\n",
       "  (backbone): IntermediateLayerGetter(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): DeepLabHead(\n",
       "    (0): ASPP(\n",
       "      (convs): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (1): ASPPConv(\n",
       "          (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(1, 1), padding=(12, 12), dilation=(12, 12), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (2): ASPPConv(\n",
       "          (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(1, 1), padding=(24, 24), dilation=(24, 24), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (3): ASPPConv(\n",
       "          (0): Conv2d(2048, 256, kernel_size=(3, 3), stride=(1, 1), padding=(36, 36), dilation=(36, 36), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "        (4): ASPPPooling(\n",
       "          (0): AdaptiveAvgPool2d(output_size=1)\n",
       "          (1): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (3): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (project): Sequential(\n",
       "        (0): Conv2d(1280, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "        (3): Dropout(p=0.5, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): ReLU()\n",
       "    (4): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEED = 3\n",
    "\n",
    "if IN_COLAB:\n",
    "    root = osp.join(\n",
    "        DATA_PATH, 'Checkpoints/deeplabv3_resnet50_lr1e-01_wd5e-04_bs40_ep80_seed1')\n",
    "    \n",
    "else:\n",
    "    root = osp.join(\n",
    "        os.environ['DATA_PATH'], 'cciw/logs/cmp-dataset/train_v120/deeplabv3_resnet50/lr1e-01/wd5e-04/bs40/ep80/seed%d/checkpoint' % SEED)\n",
    "    #root = osp.join(\n",
    "    #    os.environ['DATA_PATH'], 'cciw/logs/cmp-dataset/deeplab/trainval_v120/deeplabv3_resnet50/lr1e-01/wd5e-04/bs40/ep80/seed1/checkpoint/')\n",
    "\n",
    "ckpt_file = 'deeplabv3_resnet50_lr1e-01_wd5e-04_bs40_ep80_seed%d_epoch79.ckpt' % SEED\n",
    "#ckpt_file = 'deeplabv3_resnet50_lr1e-01_wd5e-04_bs40_ep80_seed%d_epoch40.ckpt' % SEED\n",
    "\n",
    "model_to_load = osp.join(root, ckpt_file)\n",
    "                         \n",
    "print('Loading', model_to_load)                         \n",
    "         \n",
    "checkpoint = torch.load(model_to_load)\n",
    "                        \n",
    "train_loss = checkpoint['trn_loss']\n",
    "val_loss = checkpoint['val_loss']\n",
    "print('==> Resuming from checkpoint..')\n",
    "net = checkpoint['net']\n",
    "last_epoch = checkpoint['epoch']\n",
    "torch.set_rng_state(checkpoint['rng_state'])\n",
    "\n",
    "# later appended to figure filenames\n",
    "model_stem = ckpt_file.split('.')[0]\n",
    "\n",
    "print('Loaded model %s trained to epoch ' % model_stem, last_epoch)\n",
    "print(\n",
    "    'Cross-entropy loss {:.4f} for train set, {:.4f} for validation set'.format(train_loss, val_loss))\n",
    "\n",
    "sig = nn.Sigmoid()  # initializes a sigmoid function\n",
    "\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from fcn import FCN8s\n",
    "net = FCN8s(n_class=1).to(device)\n",
    "\n",
    "from apex import amp\n",
    "net = amp.initialize(net, opt_level='O3')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/gallowaa/cciw/logs/cmp-dataset/train_v120/\n",
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/scratch/gallowaa/cciw/logs/cmp-dataset/train_v120/deeplabv3_resnet50/lr1e-01/wd5e-04/bs40/ep80/seed1/checkpoint/deeplabv3_resnet50_lr1e-01_wd5e-04_bs40_ep80_seed1_epoch79.ckpt',\n",
       " '/scratch/gallowaa/cciw/logs/cmp-dataset/train_v120/deeplabv3_resnet50/lr1e-01/wd5e-04/bs40/ep80/seed2/checkpoint/deeplabv3_resnet50_lr1e-01_wd5e-04_bs40_ep80_seed2_epoch79.ckpt',\n",
       " '/scratch/gallowaa/cciw/logs/cmp-dataset/train_v120/deeplabv3_resnet50/lr1e-01/wd5e-04/bs40/ep80/seed3/checkpoint/deeplabv3_resnet50_lr1e-01_wd5e-04_bs40_ep80_seed3_epoch79.ckpt']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root = osp.join(os.environ['DATA_PATH'], 'cciw/logs/cmp-dataset/train_v120/')\n",
    "print(root)\n",
    "\n",
    "#files = glob.glob(root + '*/*/*/*/*/*/checkpoint/*epoch79.ckpt')\n",
    "files = glob.glob(root + 'deeplab*/*/*/*/*/*/checkpoint/*epoch79.ckpt')\n",
    "\n",
    "print(len(files))\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "n89QOGLxgNyS",
    "outputId": "904c901b-f3e2-4a84-afd8-27eaad8aadf7"
   },
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    root = osp.join(DATA_PATH, 'Checkpoints/fcn8slim_lr1e-03_wd5e-04_bs32_ep50_seed1')\n",
    "else:\n",
    "    pass\n",
    "    #root = '/scratch/gallowaa/cciw/logs/v1.0.1-debug/fcn8s/lr1e-03/wd5e-04/bs25/ep80/seed4/checkpoint' # b\n",
    "    #root = '/scratch/gallowaa/cciw/logs/v111/trainval/fcn8s/lr1e-03/wd5e-04/bs40/ep80/seed2/checkpoint/' # d\n",
    "\n",
    "#ckpt_file = 'fcn8s_lr1e-03_wd5e-04_bs25_ep80_seed4_epoch70.ckpt' # b\n",
    "#ckpt_file = 'fcn8s_lr1e-03_wd5e-04_bs40_ep80_seed2amp_epoch79.pt' # d\n",
    "\n",
    "\"\"\"Feel free to try these other checkpoints later after running epoch40 to get a \n",
    "feel for how the evaluation metrics change when model isn't trained as long.\"\"\"\n",
    "\n",
    "#checkpoint = torch.load(osp.join(root, ckpt_file))\n",
    "f = 2\n",
    "checkpoint = torch.load(files[f])\n",
    "train_loss = checkpoint['trn_loss']\n",
    "val_loss = checkpoint['val_loss']\n",
    "print('==> Resuming from checkpoint..')\n",
    "\n",
    "net = checkpoint['net']\n",
    "'''\n",
    "# AMP\n",
    "net.load_state_dict(checkpoint['net'])\n",
    "amp.load_state_dict(checkpoint['amp'])\n",
    "'''\n",
    "last_epoch = checkpoint['epoch'] + 1\n",
    "torch.set_rng_state(checkpoint['rng_state'])\n",
    "\n",
    "# later appended to figure filenames\n",
    "#model_stem = ckpt_file.split('.')[0]\n",
    "model_stem = files[f].split('/')[-1].split('.')[0]\n",
    "\n",
    "print('Loaded model %s trained to epoch ' % model_stem, last_epoch)\n",
    "print('Cross-entropy loss {:.4f} for train set, {:.4f} for validation set'.format(train_loss, val_loss))\n",
    "\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#net = amp.initialize(net, opt_level='O3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UnzXpijsgNzC"
   },
   "outputs": [],
   "source": [
    "sig = nn.Sigmoid()  # initializes a sigmoid function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ne_OPnTPgNzh"
   },
   "source": [
    "## 7. i) Visualize Predictions on Whole Images\n",
    "\n",
    "Here we manually load and preprocess the original images and png masks using OpenCV.\n",
    "\n",
    "`root_path` -- will also be used in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "rouYvCy3gNzj",
    "outputId": "0cd6d473-73c3-4f2e-97ca-df03fbb55f7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "if IN_COLAB:\n",
    "    root_path = osp.join(DATA_PATH, 'ADIG_Labelled_Dataset/Test/Lab/')\n",
    "else:\n",
    "    root_path = '/scratch/ssd/gallowaa/cciw/dataset_raw/Test/WHERD/all/'\n",
    "    \n",
    "jpeg_files = glob.glob(root_path + '*.jpg')\n",
    "png_files = glob.glob(root_path + '*_final.png')\n",
    "\n",
    "jpeg_files.sort()\n",
    "png_files.sort()\n",
    "\n",
    "# Both should equal 16 for all WHERD dataset\n",
    "print(len(jpeg_files)) \n",
    "print(len(png_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/scratch/ssd/gallowaa/cciw/dataset_raw/Test/WHERD/all/WHERD_1353_2019-10-30_GoPro-8.jpg',\n",
       " '/scratch/ssd/gallowaa/cciw/dataset_raw/Test/WHERD/all/WHERD_1355_2019-10-30_GoPro-5.jpg',\n",
       " '/scratch/ssd/gallowaa/cciw/dataset_raw/Test/WHERD/all/WHERD_FS1355_2019-11-03_FishSens-1.jpg',\n",
       " '/scratch/ssd/gallowaa/cciw/dataset_raw/Test/WHERD/all/WHERD_FSHP131642_2019-11-03_FishSens-4.jpg',\n",
       " '/scratch/ssd/gallowaa/cciw/dataset_raw/Test/WHERD/all/WHERD_FSHP131645_2019-11-03_FishSens-6.jpg',\n",
       " '/scratch/ssd/gallowaa/cciw/dataset_raw/Test/WHERD/all/WHERD_FSHP131649_2019-11-03_FishSens-2.jpg',\n",
       " '/scratch/ssd/gallowaa/cciw/dataset_raw/Test/WHERD/all/WHERD_FSLEE06022_2019-11-03_FishSens-1.jpg',\n",
       " '/scratch/ssd/gallowaa/cciw/dataset_raw/Test/WHERD/all/WHERD_FSLEE06037_2019-11-03_FishSens-1.jpg',\n",
       " '/scratch/ssd/gallowaa/cciw/dataset_raw/Test/WHERD/all/WHERD_HP115410_2019-10-30_GoPro-3.jpg',\n",
       " '/scratch/ssd/gallowaa/cciw/dataset_raw/Test/WHERD/all/WHERD_HP131641_2019-11-03_GoPro-13.jpg',\n",
       " '/scratch/ssd/gallowaa/cciw/dataset_raw/Test/WHERD/all/WHERD_LEE06021_2019-10-30_GoPro-3.jpg',\n",
       " '/scratch/ssd/gallowaa/cciw/dataset_raw/Test/WHERD/all/WHERD_LEE06031_2019-10-30_GoPro-2.jpg',\n",
       " '/scratch/ssd/gallowaa/cciw/dataset_raw/Test/WHERD/all/WHERD_LEE06032_2019-11-03_GoPro-3.jpg',\n",
       " '/scratch/ssd/gallowaa/cciw/dataset_raw/Test/WHERD/all/WHERD_LEE06033_2019-10-30_GoPro-8.jpg',\n",
       " '/scratch/ssd/gallowaa/cciw/dataset_raw/Test/WHERD/all/WHERD_LEE06070_2019-10-30_GoPro-1.jpg',\n",
       " '/scratch/ssd/gallowaa/cciw/dataset_raw/Test/WHERD/all/WHERD_LEE06075_2019-10-30_GoPro-11.jpg']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jpeg_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Set to True to save the model predictions in PNG format, \n",
    "otherwise proceed to predict biomass without saving images\"\"\"\n",
    "PLOT = False\n",
    "SAVE_PREDICTIONS = False\n",
    "\n",
    "if SAVE_PREDICTIONS:\n",
    "    prediction_path = ''\n",
    "    for t in model_to_load.split('/')[:-1]:\n",
    "    #for t in files[f].split('/')[:-1]:\n",
    "        prediction_path += t + '/'\n",
    "\n",
    "    prediction_path = osp.join(prediction_path, 'WHERD')\n",
    "\n",
    "    if not osp.exists(prediction_path):\n",
    "        os.mkdir(prediction_path)\n",
    "\n",
    "    print(prediction_path)\n",
    "    \n",
    "    # src is the training dataset, tgt is the testing dataset\n",
    "    src = 'train_v120'\n",
    "    tgt = 'WHERD'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fontsize = 16\n",
    "\n",
    "left = 0.02  # the left side of the subplots of the figure\n",
    "right = 0.98   # the right side of the subplots of the figure\n",
    "bottom = 0.05  # the bottom of the subplots of the figure\n",
    "top = 0.95     # the top of the subplots of the figure\n",
    "wspace = 0.15  # the amount of width reserved for space between subplots,\n",
    "# expressed as a fraction of the average axis width\n",
    "hspace = 0.1  # the amount of height reserved for space between subplots,\n",
    "# expressed as a fraction of the average axis height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from task_3_utils import img_to_nchw_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_scales = np.array([157,  # 1353\n",
    "                          65,  # 1355\n",
    "                          33,  # FS1355\n",
    "                          40,  # FSHP131642\n",
    "                          37,  # FSHP131645\n",
    "                          100,  # FSHP131649 (55-115)\n",
    "                          23,  # FSLEE06022\n",
    "                          80,  # FSLEE06037 (40, 80, 110)\n",
    "                          125,  # HP115410\n",
    "                          100,  # HP131641\n",
    "                          150,  # LEE06021\n",
    "                          32,  # LEE06031 # (30-32, 60, 130)\n",
    "                          100,  # LEE06032\n",
    "                          100,  # LEE06033\n",
    "                          100,  # LEE06070\n",
    "                          60])  # LEE06075"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157\n",
      "65\n",
      "33\n",
      "40\n",
      "37\n",
      "100\n",
      "23\n",
      "80\n",
      "125\n",
      "100\n",
      "150\n",
      "32\n",
      "100\n",
      "100\n",
      "100\n",
      "60\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(search_scales)):\n",
    "    print(search_scales[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FSLEE06022\n",
      "20 0.309807151556015\n",
      "25 0.3077235817909241\n",
      "30 0.1907651275396347\n",
      "35 0.14062562584877014\n",
      "40 0.09385527670383453\n",
      "45 0.07593048363924026\n",
      "50 0.054370611906051636\n",
      "55 0.02905425988137722\n",
      "60 0.0057550836354494095\n",
      "65 0.010719843208789825\n",
      "70 0.01070113480091095\n",
      "75 0.00012910444638691843\n",
      "80 0.0032458724454045296\n",
      "85 0.007925771176815033\n",
      "90 0.006963593885302544\n",
      "95 0.011155098676681519\n",
      "100 0.010659490711987019\n",
      "105 0.025866156443953514\n",
      "110 0.05109870806336403\n",
      "115 0.040722284466028214\n",
      "120 0.044854581356048584\n",
      "125 0.058491963893175125\n",
      "130 0.05969184637069702\n",
      "135 0.07352831959724426\n",
      "140 0.08691459894180298\n",
      "145 0.08673032373189926\n"
     ]
    }
   ],
   "source": [
    "# line search for optimal scale\n",
    "i = 6\n",
    "scales = np.arange(20, 150, 5)\n",
    "#scales = np.arange(30, 65, 1)\n",
    "image_stem = jpeg_files[i].split('/')[-1].split('.')[0]\n",
    "bgr_lab = cv2.imread(osp.join(root_path, png_files[i]))\n",
    "bgr_img = cv2.imread(osp.join(root_path, jpeg_files[i]))\n",
    "labc = cv2.cvtColor(bgr_lab, cv2.COLOR_BGR2RGB)\n",
    "imgc = cv2.cvtColor(bgr_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "print(image_stem.split('_')[1])\n",
    "for scale_percent in scales:\n",
    "    \n",
    "    img = resize(imgc, scale_percent)\n",
    "    lab = resize(labc, scale_percent)\n",
    "    \n",
    "    nchw_tensor = img_to_nchw_tensor(img, device)\n",
    "    with torch.no_grad():\n",
    "        pred = sig(net(nchw_tensor)['out'])\n",
    "    pred_np = pred.detach().cpu().numpy()\n",
    "    mask = np.zeros((lab.shape[0], lab.shape[1]), dtype='float32')\n",
    "    mask[lab[:, :, 0] == 128] = 1\n",
    "    pred_np = pred_np.squeeze()\n",
    "    targets = torch.LongTensor(mask)\n",
    "    targets = targets.to(device)\n",
    "    iou = eval_binary_iou(pred, targets).item()\n",
    "    print(scale_percent, iou)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find laser beams in image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1080, 1920, 3)\n",
      "(1080, 1920, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(720, 1280, 3)\n",
      "(1080, 1920, 3)\n",
      "(1080, 1920, 3)\n",
      "(1080, 1920, 3)\n",
      "(1080, 1920, 3)\n",
      "(1080, 1920, 3)\n",
      "(1080, 1920, 3)\n",
      "(1080, 1920, 3)\n",
      "(1080, 1920, 3)\n"
     ]
    }
   ],
   "source": [
    "# meta-parameters for algorithm to find two red laser dots\n",
    "# red_threshold is out of 255, match_threshold out of 1.0 for template matching\n",
    "# red_threshold, match_threshold = 200, 0.2 # 10 correct\n",
    "intensity_threshold, match_threshold = 150, 0.3  # 10\n",
    "\n",
    "\"\"\"C - Constant subtracted from the mean or weighted mean (see the details below). \n",
    "Normally, it is positive but may be zero or negative as well.\"\"\"\n",
    "C = -15\n",
    "\n",
    "\"\"\"Block size - Size of a pixel neighborhood that is used to calculate a threshold \n",
    "value for the pixel: 3, 5, 7, and so on.\"\"\"\n",
    "block_size = 201\n",
    "\n",
    "# noise removing kernel\n",
    "kernel = np.ones((5, 5), np.uint8)\n",
    "dilate_kernel = np.ones((6, 6), np.uint8)\n",
    "\n",
    "j = 0\n",
    "laser_distances = []\n",
    "\n",
    "for i in range(len(jpeg_files)):\n",
    "    f = jpeg_files[i]\n",
    "    #if f.split('/')[-1] in include_list:\n",
    "    im = cv.imread(f)\n",
    "    print(im.shape)\n",
    "    #outfile = os.path.join(save_path, f.split('/')[-1])\n",
    "\n",
    "    im = cv.imread(f)  # im will be in BGR format\n",
    "    im_thresh = im.copy()\n",
    "\n",
    "    im_ycrbc = cv.cvtColor(im, cv.COLOR_BGR2YCrCb)\n",
    "\n",
    "    Y  = im_ycrbc[:, :, 0] # Separate pixel intensity Y from \n",
    "    Cr = im_ycrbc[:, :, 1] # red-difference chroma \n",
    "    Cb = im_ycrbc[:, :, 2] # blue-difference components\n",
    "\n",
    "    im_thresh[Cr < intensity_threshold] = 0\n",
    "\n",
    "    cts, bin_edges = np.histogram(im_ycrbc[:, :, 2])\n",
    "    red_range = bin_edges.max() - bin_edges.min()\n",
    "\n",
    "    Cr_th = cv.adaptiveThreshold(\n",
    "        Cr, 255, cv.ADAPTIVE_THRESH_GAUSSIAN_C, cv.THRESH_BINARY, 201, C)\n",
    "\n",
    "    # morphology and connected components\n",
    "    opening = cv.morphologyEx(Cr_th, cv.MORPH_OPEN, kernel, iterations=2)\n",
    "    sure_bg = cv.dilate(opening, dilate_kernel, iterations=3)\n",
    "\n",
    "    # Marker labelling\n",
    "    ret, markers = cv.connectedComponents(sure_bg)\n",
    "\n",
    "    # get the cluster IDs and number of pixels in each\n",
    "    ids, cts = np.unique(markers, return_counts=True)\n",
    "\n",
    "    # only keep the largest two clusters by pixel count\n",
    "    largest_blobs = ids[1:][np.argsort(cts[1:])][-2:]\n",
    "\n",
    "    if len(largest_blobs) == 2:\n",
    "        mask = markers == largest_blobs[0]\n",
    "        mask |= markers == largest_blobs[1]\n",
    "        markers[np.invert(mask)] = 0\n",
    "\n",
    "    # we can now encode clusters with the same value as\n",
    "    # they should be physically separated\n",
    "    markers[markers > 0] = 1\n",
    "\n",
    "    # get numpy array of coordinates of non-zero elements\n",
    "    coords = cv.findNonZero(markers.astype('uint8'))\n",
    "\n",
    "    if coords is not None:\n",
    "        kmeans = KMeans(n_clusters=2, random_state=0).fit(coords.squeeze())\n",
    "        # draw the two clusters\n",
    "        for pt in kmeans.cluster_centers_:\n",
    "            _ = cv.circle(im, (int(pt[0]), int(pt[1])), 50, (0, 255, 0), 2)\n",
    "\n",
    "        # compute the distance in pixels between the two laser beams\n",
    "        d = np.linalg.norm(\n",
    "            kmeans.cluster_centers_[0] - kmeans.cluster_centers_[1])\n",
    "    else:\n",
    "        d = -1  # do not change scale if -1\n",
    "    \n",
    "    laser_distances.append(d)\n",
    "    '''\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(14, 6))\n",
    "    title_str = '%d Ratio %.3f - dots are dist %d of %d px' % (j, d/im.shape[1], d, im.shape[1])\n",
    "    ax.imshow(im)\n",
    "    ax.set_title(title_str)\n",
    "    plt.show()\n",
    "    '''\n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "jpeg_file_resolutions = []\n",
    "for i in range(len(jpeg_files)):\n",
    "    jpeg_file_resolutions.append(np.prod(cv.imread(jpeg_files[i]).shape[:2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.61158741,  0.76348841,  0.71747365,  0.87254581,  0.4749003 ,\n",
       "        0.61373774,  1.        ,  0.78011089,  0.5597033 ,  0.74120516,\n",
       "        0.54149451,  0.58806742,  0.65102339,  0.51385305, -0.00146881,\n",
       "        0.62179415])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "laser_distances = np.round(np.asarray(laser_distances), 3)\n",
    "#mask = laser_distances > 0\n",
    "#lasers = laser_distances[mask]\n",
    "lasers = laser_distances / laser_distances.max()\n",
    "lasers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "jpeg_file_resolutions = np.asarray(jpeg_file_resolutions)\n",
    "jpeg_file_resolutions = jpeg_file_resolutions / float(jpeg_file_resolutions.max())\n",
    "res = jpeg_file_resolutions.copy()#[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide by the resolution so that less resolution = larger distance\n",
    "d = 1 / (lasers / res)\n",
    "d = d / d.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8.4000e+01,  6.7000e+01,  3.2000e+01,  2.6000e+01,  4.8000e+01,\n",
       "        3.7000e+01,  2.3000e+01,  2.9000e+01,  9.2000e+01,  6.9000e+01,\n",
       "        9.5000e+01,  8.7000e+01,  7.9000e+01,  1.0000e+02, -3.4984e+04,\n",
       "        8.3000e+01])"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(d, 2)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fd96470b8d0>]"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.round(d, 2)*100)\n",
    "#plt.plot(search_scales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([157,  65,  33,  40,  37, 100,  23,  80, 125, 100, 150,  32, 100,\n",
       "       100, 100,  60])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    84,     67,     31,     26,     48,     37,     22,     29,\n",
       "           91,     69,     94,     87,     78,    100, -34984,     82])"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "laser_scales = (d * 100).astype(int)\n",
    "laser_scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84\n",
      "67\n",
      "31\n",
      "26\n",
      "48\n",
      "37\n",
      "22\n",
      "29\n",
      "91\n",
      "69\n",
      "94\n",
      "87\n",
      "78\n",
      "100\n",
      "-34984\n",
      "82\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(laser_scales)):\n",
    "    print(laser_scales[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize(image, pct):\n",
    "    width = int(image.shape[1] * pct / 100)\n",
    "    height = int(image.shape[0] * pct / 100)\n",
    "    return cv2.resize(image, (width, height))  # resize image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "cxaxZTfigNzs",
    "outputId": "394b6282-16e5-4596-dd56-4272301189a4",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, 0.2295\n",
      "1, 0.4012\n",
      "2, 0.2567\n",
      "3, 0.0000\n",
      "4, 0.0625\n",
      "5, 0.0000\n",
      "6, 0.2596\n",
      "7, 0.3466\n",
      "8, 0.2484\n",
      "9, 0.4125\n",
      "10, 0.3448\n",
      "11, 0.2050\n",
      "12, 0.2353\n",
      "13, 0.0205\n",
      "14, 0.0000\n",
      "15, 0.2125\n",
      "Average  0.2022\n"
     ]
    }
   ],
   "source": [
    "iou_list = []\n",
    "#scale_percent = 100\n",
    "\n",
    "for i in range(len(jpeg_files)):\n",
    "    image_stem = jpeg_files[i].split('/')[-1].split('.')[0]\n",
    "    bgr_lab = cv2.imread(osp.join(root_path, png_files[i]))\n",
    "    bgr_img = cv2.imread(osp.join(root_path, jpeg_files[i]))\n",
    "    lab = cv2.cvtColor(bgr_lab, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.cvtColor(bgr_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    scale_percent = laser_scales[i] if laser_scales[i] > 0 else 100\n",
    "    #scale_percent = search_scales[i]\n",
    "    img = resize(img, scale_percent)\n",
    "    lab = resize(lab, scale_percent)\n",
    "\n",
    "    nchw_tensor = img_to_nchw_tensor(img, device)\n",
    "    with torch.no_grad():\n",
    "        pred = sig(net(nchw_tensor)['out'])\n",
    "    pred_np = pred.detach().cpu().numpy().squeeze()\n",
    "    mask = np.zeros((lab.shape[0], lab.shape[1]), dtype='float32')\n",
    "    mask[lab[:, :, 0] == 128] = 1\n",
    "    \n",
    "    targets = torch.LongTensor(mask)\n",
    "    targets = targets.to(device)\n",
    "    iou = eval_binary_iou(pred, targets).item()\n",
    "    '''\n",
    "    p_one_hot, t_one_hot = mask_and_preds_to_1hot(pred_np, mask)\n",
    "\n",
    "    iou = jsc(p_one_hot.reshape(1, -1),\n",
    "              t_one_hot.reshape(1, -1), average='samples')\n",
    "    '''\n",
    "    iou_list.append(iou)\n",
    "    print('%d, %.4f' % (i, iou))\n",
    "\n",
    "    if PLOT:\n",
    "        # plt.close('all')\n",
    "        fig, axes = plt.subplots(1, 1, figsize=(19.20, 10.80))\n",
    "        p = (pred_np * 255).astype('uint8')\n",
    "        src2 = np.zeros((p.shape[0], p.shape[1], 3), np.uint8)\n",
    "        src2[:, :, 2] = p\n",
    "        dst = cv2.addWeighted(img, 0.5, src2, 0.5, 0)\n",
    "        axes.imshow(dst)\n",
    "        axes.axis('off')\n",
    "        plt.tight_layout()\n",
    "    if SAVE_PREDICTIONS:\n",
    "        filename = src + '__' + image_stem + '__' + model_stem + \\\n",
    "            '_scale%d_iou_%.4f' % (scale_percent, iou)\n",
    "        out_file = osp.join(prediction_path, filename)\n",
    "        fig.savefig(out_file + '.jpg', format='jpeg')\n",
    "        \n",
    "print(\"Average \", np.round(np.asarray(iou_list).mean(), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "iou = jsc(p_one_hot.reshape(1, -1),\n",
    "          t_one_hot.reshape(1, -1), average='samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0 0.3349668479370011\n",
    "1 0.2730633762824413\n",
    "2 0.08701870141603765\n",
    "3 0.6963940582031034\n",
    "4 0.7138648992434893\n",
    "5 0.2700486397208539\n",
    "6 0.6887657497196386\n",
    "7 0.943557536979472\n",
    "8 0.42190770560009877\n",
    "9 0.8012735566943816\n",
    "10 0.9509239510349701\n",
    "11 0.9770874858352112\n",
    "12 0.8296693915528675\n",
    "13 0.44408103316978087\n",
    "14 0.9843869248270862\n",
    "15 0.8617212302299858\n",
    "mean 0.6424206930279013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_and_preds_to_1hot(p, y):\n",
    "    \n",
    "    # labels y to one hot encoding\n",
    "    y_1hot = np.zeros((2, y.shape[0], y.shape[1]))\n",
    "    y_1hot[1, :, :][y == 1] = 1\n",
    "    y_1hot[0, :, :][y == 0] = 1\n",
    "\n",
    "    # predictions p to one hot encoding\n",
    "    p_1hot = np.zeros((2, p.shape[0], p.shape[1]))\n",
    "    p_1hot[1, :, :][p.squeeze().round() == 1] = 1\n",
    "    p_1hot[0, :, :][p.squeeze().round() == 0] = 1\n",
    "    \n",
    "    return p_1hot, y_1hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOM: Figure 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 4\n",
    "\n",
    "image_stem = jpeg_files[i].split('/')[-1].split('.')[0]\n",
    "bgr_lab = cv2.imread(osp.join(root_path, png_files[i]))\n",
    "bgr_img = cv2.imread(osp.join(root_path, jpeg_files[i]))\n",
    "lab = cv2.cvtColor(bgr_lab, cv2.COLOR_BGR2RGB)\n",
    "img = cv2.cvtColor(bgr_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "\n",
    "scale_percent = 38\n",
    "#for scale_percent in scales:\n",
    "width = int(imgc.shape[1] * scale_percent / 100)\n",
    "height = int(imgc.shape[0] * scale_percent / 100)\n",
    "img = cv2.resize(imgc, (width, height)) # resize image\n",
    "lab = cv2.resize(labc, (width, height)) # resize image\n",
    "\n",
    "nchw_tensor = img_to_nchw_tensor(img, device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    pred = sig(net(nchw_tensor)['out'])\n",
    "\n",
    "pred_np = pred.detach().cpu().numpy().squeeze()\n",
    "\n",
    "mask = np.zeros((lab.shape[0], lab.shape[1]), dtype='float32')\n",
    "mask[lab[:, :, 0] == 128] = 1\n",
    "\n",
    "'''\n",
    "targets = torch.LongTensor(mask)\n",
    "targets = targets.to(device)\n",
    "#print(targets.shape)\n",
    "iou = eval_binary_iou(pred, targets).item()\n",
    "'''\n",
    "\n",
    "p_one_hot, t_one_hot = mask_and_preds_to_1hot(pred_np, mask)\n",
    "\n",
    "iou = jsc(p_one_hot.reshape(1, -1),\n",
    "          t_one_hot.reshape(1, -1), average='samples')\n",
    "\n",
    "#iou_list.append(iou)\n",
    "print(i, iou)\n",
    "\n",
    "#if PLOT:\n",
    "\n",
    "plt.close('all')\n",
    "fig, axes = plt.subplots(1, 1, figsize=(19.20, 10.80))\n",
    "p = (pred_np * 255).astype('uint8')\n",
    "src2 = np.zeros((p.shape[0], p.shape[1], 3), np.uint8)\n",
    "src2[:, :, 2] = p\n",
    "dst = cv2.addWeighted(img, 0.75, src2, 0.5, 0)\n",
    "axes.imshow(dst)\n",
    "axes.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "'''\n",
    "if SAVE_PREDICTIONS:\n",
    "    filename = src + '__' + image_stem + '__' + model_stem + '_scale%d_iou_%.4f' % (scale_percent, iou)\n",
    "    out_file = osp.join(prediction_path, filename)\n",
    "    fig.savefig(out_file + '.jpg', format='jpeg')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "fig, axes = plt.subplots(1, 1, figsize=(12.80, 7.20))\n",
    "p = (pred_np * 255).astype('uint8')\n",
    "src2 = np.zeros((p.shape[0], p.shape[1], 3), np.uint8)\n",
    "src2[:, :, 2] = p\n",
    "dst = cv2.addWeighted(img, 0.75, src2, 0.75, 0.25)\n",
    "axes.imshow(dst)\n",
    "axes.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'fig5-' + src + '-' + tgt + '__' + image_stem + '__' + model_stem\n",
    "out_file = osp.join(prediction_path, filename)\n",
    "fig.savefig(out_file + '_scale%d.jpg' % scale_percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "task_3_evaluate_checkpoint_in_colab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
