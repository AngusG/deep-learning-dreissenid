{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0UXd9CuzgNx2"
   },
   "source": [
    "# Count Mussels in an Image\n",
    "\n",
    "Demonstrates image pre-processing, prediction and validation statistics. But first, some preliminaries...\n",
    "\n",
    "__Note:__ To maintain a high priority Colab user status such that sufficient GPU resources are available in the future, ensure to free the runtime when finished running this notebook. This can be done using 'Runtime > Manage Sessions' and click 'Terminate'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if notebook is running in Colab or local workstation\n",
    "import sys\n",
    "\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    !ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
    "    !pip install gputil\n",
    "    !pip install psutil\n",
    "    !pip install humanize\n",
    "\n",
    "import psutil\n",
    "import humanize\n",
    "import os\n",
    "import GPUtil as GPU\n",
    "GPUs = GPU.getGPUs()\n",
    "\n",
    "try:\n",
    "    # XXX: only one GPU on Colab and isn’t guaranteed\n",
    "    gpu = GPUs[0]\n",
    "    def printm():\n",
    "        process = psutil.Process(os.getpid())\n",
    "        print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
    "        print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
    "    printm() \n",
    "\n",
    "    # Check if GPU capacity is sufficient to proceed\n",
    "    if gpu.memoryFree < 10000:\n",
    "        print(\"\\nInsufficient memory! Some cells may fail. Please try restarting the runtime using 'Runtime → Restart Runtime...' from the menu bar. If that doesn't work, terminate this session and try again later.\")\n",
    "    else:\n",
    "        print('\\nGPU memory is sufficient to proceeed.')\n",
    "except:\n",
    "    print('Select the Runtime → \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
    "    print('and then re-execute this cell.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "4pzGY41DgyyQ",
    "outputId": "6bebf55b-f9f5-4a59-abdd-4c101878f4d1"
   },
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    DATA_PATH = r'/content/drive/My Drive/Data'\n",
    "    \n",
    "    # cd into git repo so python can find utils\n",
    "    %cd '/content/drive/My Drive/cciw-zebra-mussel/predict'\n",
    "\n",
    "    sys.path.append('/content/drive/My Drive')\n",
    "    \n",
    "    # clone repo, install packages not installed by default\n",
    "    !pip install pydensecrf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bneyBxcYgNx7"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "\n",
    "import glob\n",
    "\n",
    "# for manually reading high resolution images\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# for comparing predictions to lab analysis data frames\n",
    "import pandas as pd\n",
    "\n",
    "# for plotting\n",
    "import matplotlib\n",
    "# enable LaTeX style fonts\n",
    "matplotlib.rc('text', usetex=True)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rc('text', usetex=True)\n",
    "plt.rc('font', family='serif')\n",
    "\n",
    "# pytorch core library\n",
    "import torch\n",
    "# pytorch neural network functions\n",
    "from torch import nn\n",
    "# pytorch dataloader\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# for post-processing model predictions by conditional random field \n",
    "import pydensecrf.densecrf as dcrf\n",
    "import pydensecrf.utils as utils\n",
    "\n",
    "from tqdm import tqdm  # progress bar\n",
    "\n",
    "# evaluation metrics\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import jaccard_score as jsc\n",
    "\n",
    "# local imports (files provided by this repo)\n",
    "import transforms as T\n",
    "\n",
    "# various helper functions, metrics that can be evaluated on the GPU\n",
    "from task_3_utils import evaluate, evaluate_loss, eval_binary_iou, pretty_image\n",
    "\n",
    "# Custom dataloader for rapidly loading images from a single LMDB file\n",
    "from folder2lmdb import VOCSegmentationLMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Confim that this cell prints \"Found GPU, cuda\". If not, select \"GPU\" as \n",
    "\"Hardware Accelerator\" under the \"Runtime\" tab of the main menu.\n",
    "\"\"\"\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print('Found GPU,', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n4P0mldogNyQ"
   },
   "source": [
    "## 1. Load a pre-trained model checkpoint\n",
    "\n",
    "The architecture is fully-convolutional network (FCN) 8s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "n89QOGLxgNyS",
    "outputId": "904c901b-f3e2-4a84-afd8-27eaad8aadf7"
   },
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    root = osp.join(DATA_PATH, 'Checkpoints/fcn8slim_lr1e-03_wd5e-04_bs32_ep50_seed1')\n",
    "else:\n",
    "    root = '/scratch/gallowaa/cciw/logs/lab-v1.0.0/fcn8slim/lr1e-03/wd5e-04/bs32/ep50/seed1/checkpoint' # a\n",
    "    #root = '/scratch/gallowaa/cciw/logs/v1.0.1-debug/fcn8s/lr1e-03/wd5e-04/bs25/ep80/seed4/checkpoint' # b\n",
    "    #root = '/scratch/gallowaa/cciw/logs/v1.1.0-debug/fcn8s/lr1e-03/wd5e-04/bs25/ep80/seed9/checkpoint/' # c\n",
    "    #root = '/scratch/gallowaa/cciw/logs/v111/trainval/fcn8s/lr1e-03/wd5e-04/bs40/ep80/seed2/checkpoint/' # d\n",
    "    #root = '/scratch/gallowaa/cciw/logs/v111/trainval/fcn8slim/lr1e-04/wd5e-04/bs40/ep80/seed1/checkpoint/' # e\n",
    "\n",
    "ckpt_file = 'fcn8slim_lr1e-03_wd5e-04_bs32_ep50_seed1_epoch40.ckpt' # a\n",
    "#ckpt_file = 'fcn8s_lr1e-03_wd5e-04_bs25_ep80_seed4_epoch70.ckpt' # b\n",
    "#ckpt_file = 'fcn8s_lr1e-03_wd5e-04_bs25_ep80_seed9_epoch10.ckpt'\n",
    "#ckpt_file = 'fcn8s_lr1e-03_wd5e-04_bs40_ep80_seed2amp_epoch79.pt' # d\n",
    "#ckpt_file = 'fcn8slim_lr1e-04_wd5e-04_bs40_ep80_seed1amp_epoch79.pt' # e\n",
    "\n",
    "\"\"\"Feel free to try these other checkpoints later after running epoch40 to get a \n",
    "feel for how the evaluation metrics change when model isn't trained as long.\"\"\"\n",
    "#ckpt_file = 'fcn8slim_lr1e-03_wd5e-04_bs32_ep50_seed1_epoch10.ckpt'\n",
    "#ckpt_file = 'fcn8slim_lr1e-03_wd5e-04_bs32_ep50_seed1_epoch0.ckpt'\n",
    "\n",
    "checkpoint = torch.load(osp.join(root, ckpt_file))\n",
    "train_loss = checkpoint['trn_loss']\n",
    "val_loss = checkpoint['val_loss']\n",
    "print('==> Resuming from checkpoint..')\n",
    "\n",
    "net = checkpoint['net']\n",
    "last_epoch = checkpoint['epoch']\n",
    "torch.set_rng_state(checkpoint['rng_state'])\n",
    "'''\n",
    "# AMP\n",
    "net.load_state_dict(checkpoint['net'])\n",
    "amp.load_state_dict(checkpoint['amp'])\n",
    "last_epoch = checkpoint['epoch'] + 1\n",
    "torch.set_rng_state(checkpoint['rng_state'])\n",
    "'''\n",
    "# later appended to figure filenames\n",
    "model_stem = ckpt_file.split('.')[0]\n",
    "\n",
    "print('Loaded model %s trained to epoch ' % model_stem, last_epoch)\n",
    "print('Cross-entropy loss {:.4f} for train set, {:.4f} for validation set'.format(train_loss, val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fcn import FCN8s\n",
    "net = FCN8s(n_class=1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fcn import FCN8slim\n",
    "net = FCN8slim(n_class=1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from apex import amp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-NGC_m6igNyY"
   },
   "source": [
    "## 2. Define Image Pre-Processing Transforms and Data Augmentation\n",
    "\n",
    "Here, we define transforms to be applied to input images (`inputs`) and segmentation masks (`targets`)\n",
    "on the fly as we draw mini-batches like:\n",
    "\n",
    "```\n",
    "for inputs, targets in dataloader:\n",
    "    pass\n",
    "```\n",
    "\n",
    "These transforms are documented here: https://pytorch.org/docs/stable/torchvision/transforms.html\n",
    "\n",
    "We may wish to experiment with additional ones in the future, e.g., `ColorJitter` to perturb the image colours, \n",
    "or `Grayscale` to convert the dataset to Greyscale and quantify the marginal impact of colour information on model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b_ERno-wgNya"
   },
   "outputs": [],
   "source": [
    "training_tforms = []\n",
    "\n",
    "# Randomly crop images to square 224x224\n",
    "training_tforms.append(T.RandomCrop(224)) \n",
    "\n",
    "# With probability 0.5, flip the images and masks horizontally.\n",
    "# This increases the effective size of our training set, as \n",
    "# mussels are rotation invariant.\n",
    "training_tforms.append(T.RandomHorizontalFlip(0.5)) \n",
    "\n",
    "# Similarly, flip the images and masks vertically with probability 0.5.\n",
    "training_tforms.append(T.RandomVerticalFlip(0.5))\n",
    "\n",
    "# Convert images from Python Imaging Library (PIL aka Pillow) format to PyTorch Tensor.\n",
    "training_tforms.append(T.ToTensor())\n",
    "\n",
    "\"\"\"\n",
    "T.Normalize performs: image = (image - mean) / std\n",
    "\n",
    "The first argument (a triple) to T.Normalize are the global \n",
    "RGB pixel mean values, and the second argument is their standard deviation. \n",
    "\n",
    "For a mini-batch 'inputs' comprised of N samples, \n",
    "C channels, e.g. 3 for RGB images, height H, width W, and \n",
    "inputs.shape = torch.Size([N, C, H, W]), this can be obtained using:\n",
    "\n",
    "inputs.mean(dim=(0, 2, 3)), which will output a tensor, e.g., \n",
    "tensor([0.2613, 0.2528, 0.2255]). \n",
    "\n",
    "The standard deviation can be obtained similarly with:\n",
    "inputs.std(dim=(0, 2, 3))\n",
    "\n",
    "The global values can simply be obtained by averaging over all \n",
    "mini-batches in the dataset.\n",
    "\n",
    "For the natural mussel dataset (i.e. not the Lab images), \n",
    "these global pixel values are somewhat meaningless to due \n",
    "significant changes in lighting and hue, so we simply\n",
    "pass the triple (0.5, 0.5, 0.5) for both mean and std to\n",
    "normalize the input image pixels from [0, 1] to [-1, 1]. \n",
    "This centers the images and resulting feedforward activations \n",
    "around zero and allows training to proceed more smoothly.\n",
    "\"\"\"\n",
    "training_tforms.append(T.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)))\n",
    "\n",
    "# Finally, Compose several transforms together.\n",
    "training_tforms = T.Compose(training_tforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WIZZZkGsgNyg"
   },
   "source": [
    "For validation and testing, we often want these transforms to be deterministic to be sure the model is making progress with respect to the natural image distribution. We will evaluate on fixed 250x250 patches rather than randomly cropping.\n",
    "\n",
    "For evaluating robustness, we could add `ColorJitter` and do scaling or shearing with various Affine transforms here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yTETb4fcgNyi"
   },
   "outputs": [],
   "source": [
    "test_tform = T.Compose([\n",
    "    T.CenterCrop(224),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J7DgZ6KggNyn"
   },
   "source": [
    "## 3. Create Efficient Data Loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qZHnWKZZgNyp"
   },
   "source": [
    "Specify the mini-batch size (`batch_size`) for validation, and path to serialized LMDB dataset `dataset_root`. \n",
    "\n",
    "The `batch_size` is arbitrary at test time since we aren't using `nn.BatchNorm()`, the main consideration here \n",
    "is to use the largest `batch_size` the GPU memory allows to maximize throughput. The default setting should be fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cbRHNLJCgNyr"
   },
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "\n",
    "if IN_COLAB:\n",
    "    dataset_root = osp.join(DATA_PATH, 'Lab_dataset_train_validation/LMDB/')\n",
    "else:\n",
    "    dataset_root = '/scratch/ssd/gallowaa/cciw/LMDB'\n",
    "    #dataset_root = '/scratch/ssd/gallowaa/cciw/Lab'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PHcdHi6WgNyw"
   },
   "source": [
    "The `VOCSegmentationLMDB` class was adapted from https://pytorch.org/docs/stable/torchvision/datasets.html#torchvision.datasets.VOCSegmentation\n",
    "to enable reading data from a single `*.lmdb` database which is much more efficient on conventional hard drives than randomly reading images.\n",
    "\n",
    "Note that transforms provided to the `transforms` argument apply to both input images and masks. \n",
    "The label values will be rotated accordingly as the input images, but the labels are unaffected by the normalization due to being limited to values 0/1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y4Z5FsLggNyx"
   },
   "outputs": [],
   "source": [
    "validation_set = VOCSegmentationLMDB(\n",
    "    root=osp.join(dataset_root, 'val_v101.lmdb'), transforms=test_tform)\n",
    "\n",
    "val_loader = DataLoader(validation_set, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zCVrVJpmgNy3"
   },
   "outputs": [],
   "source": [
    "training_set = VOCSegmentationLMDB(\n",
    "    root=osp.join(dataset_root, 'train_v111.lmdb'), transforms=test_tform)\n",
    "\n",
    "train_loader = DataLoader(training_set, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "esid2a7ugNy7"
   },
   "source": [
    "To compute the `pos_weight` from the dataset, uncomment the following cell.\n",
    "Note that the `batch_pos_weight` may be `inf` for a batch comprised entirely of \n",
    "masks without any mussels. Increase the `batch_size` to avoid this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sLXNyotfgNy9"
   },
   "outputs": [],
   "source": [
    "total_mussel = 0.\n",
    "total_pixels = 0.\n",
    "for idx, data in enumerate(val_loader):\n",
    "    total_mussel += (data[1] == 1).sum().float().item()\n",
    "    total_pixels += (data[1] == 0).sum().float().item()\n",
    "    print('Batch %d of %d, pos_weight=%.4f' % (idx, len(val_loader), total_mussel / total_pixels))\n",
    "print('pos_weight={:.4f}'.format(total_pixels / total_mussel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UnzXpijsgNzC"
   },
   "outputs": [],
   "source": [
    "pos_weight = torch.FloatTensor([12.4924]).to(device)\n",
    "loss_fn = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "sig = nn.Sigmoid()  # initializes a sigmoid function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "slfr2kKjgNzG"
   },
   "source": [
    "## 4. Compute training and validation cross-entropy losses\n",
    "\n",
    "to ensure model was loaded correctly and that the data were pre-processed in consistent manner w.r.t. the training script.\n",
    "\n",
    "Note: the cross-entropy loss is a proxy for what we ultimately want to measure, the intersection of the prediction and masks divided by their union."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "B1UPU2vngNzI",
    "outputId": "e792b292-9a72-4eaa-cb88-e5cc55016938"
   },
   "outputs": [],
   "source": [
    "calculate_validation_loss = evaluate_loss(net, val_loader, loss_fn, device)\n",
    "assert np.allclose(calculate_validation_loss, val_loss, atol=1e-3)\n",
    "print('\\n Validation loss of {:.4f} matches checkpoint'.format(calculate_validation_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "A9y3K27BgNzN",
    "outputId": "34070730-985b-4910-b257-988404d958e4"
   },
   "outputs": [],
   "source": [
    "# note: train loss may not match exactly \n",
    "calculate_train_loss = evaluate_loss(net, train_loader, loss_fn, device)\n",
    "\n",
    "print('\\n Calculated train loss of {:.4f}'.format(calculate_train_loss))\n",
    "print('\\n Checkpoint train loss of {:.4f}'.format(train_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HKJUfvJMgNzS"
   },
   "source": [
    "## 5. Compute the Mean Intersection-over-Union (mIoU) Score on the Validation Set\n",
    "\n",
    "The mean Intersection-over-Union (IoU), aka Jaccard Score or Jaccard Index, on the validation/test dataset is \n",
    "the main performance metric we use to evaluate semantic segmentation models.\n",
    "\n",
    "Detail: using `torch.no_grad()` saves memory if we will not be \n",
    "doing error backpropagation as intermediate activations can be \n",
    "discarded. Otherwise these are retained in GPU memory in case \n",
    "we want to compute gradients. See https://pytorch.org/docs/stable/autograd.html#torch.autograd.no_grad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "yrSSNlXOgNzT",
    "outputId": "2cc68f6d-180d-41ea-8f25-222506385190"
   },
   "outputs": [],
   "source": [
    "batch = 0\n",
    "running_iou = 0\n",
    "\n",
    "net.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    for inputs, targets in tqdm(train_loader, unit=' images', unit_scale=batch_size):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        \"\"\"Apply the sigmoid function here so that output lies in [0, 1]. \n",
    "        Previously it was applied internally by the loss_fn.\n",
    "        \n",
    "        This line does a feedforward pass, or prediction.\"\"\"\n",
    "        pred = sig(net(inputs))\n",
    "        \n",
    "        bin_iou = eval_binary_iou(pred.round(), targets)\n",
    "        \n",
    "        if (bin_iou > 0).sum() > 1:\n",
    "            iou = bin_iou[bin_iou > 0].mean().item()\n",
    "            running_iou += iou\n",
    "            batch += 1\n",
    "    running_iou = running_iou / batch\n",
    "\n",
    "print('\\n mIoU = %.4f' % running_iou)  # 0.8638 for epoch40 model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I0SKkX68gNzY"
   },
   "source": [
    "## 6. Visualize Validation Predictions on 250x250 Patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "tHtz3FrOgNzZ",
    "outputId": "1ab8e419-80ec-473b-9589-8e0862241a58"
   },
   "outputs": [],
   "source": [
    "nhwc = inputs.permute(0, 2, 3, 1).detach().cpu().numpy()\n",
    "pred_np = pred.detach().cpu().numpy()\n",
    "targets_np = targets.detach().cpu().numpy()\n",
    "\n",
    "# put pixels back into range [0, 1] for matplotlib\n",
    "nhwc = (nhwc * 0.5) + 0.5\n",
    "\n",
    "print(nhwc.shape)\n",
    "print(nhwc.min(), nhwc.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "colab_type": "code",
    "id": "ChCTI-rZgNzd",
    "outputId": "67b7bda6-fa0a-4e01-baaf-d00483215445"
   },
   "outputs": [],
   "source": [
    "j = 4  # change me! (in 0 to 45)\n",
    "\n",
    "N_PLOTS = 4\n",
    "fig, ax = plt.subplots(1, N_PLOTS, figsize=(16, 4))\n",
    "\n",
    "ax[0].imshow((nhwc[j]))\n",
    "ax[1].imshow(pred_np[j].squeeze())\n",
    "ax[2].imshow(pred_np[j].round().squeeze())\n",
    "ax[3].imshow(targets_np[j])\n",
    "\n",
    "for i in range(N_PLOTS):\n",
    "    ax[i].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ne_OPnTPgNzh"
   },
   "source": [
    "## 7. i) Visualize Predictions on Whole Images\n",
    "\n",
    "Here we manually load and preprocess the original images and png masks using OpenCV.\n",
    "\n",
    "`root_path` -- will also be used in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "rouYvCy3gNzj",
    "outputId": "0cd6d473-73c3-4f2e-97ca-df03fbb55f7a"
   },
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    root_path = osp.join(DATA_PATH, 'ADIG_Labelled_Dataset/Test/Lab/')\n",
    "else:\n",
    "    root_path = '/scratch/ssd/gallowaa/cciw/dataset_raw/Test/Lab/done/'\n",
    "    #root_path = '/scratch/ssd/gallowaa/cciw/VOCdevkit/Validation-v101-originals/'\n",
    "    #root_path = '/scratch/ssd/gallowaa/cciw/dataset_raw/Train/2018-06/land/'\n",
    "\n",
    "jpeg_files = glob.glob(root_path + '*.jpg')\n",
    "#png_files = glob.glob(root_path + '*.png') # for lab\n",
    "png_files = glob.glob(root_path + '*final.png') # for lab\n",
    "\n",
    "# in-situ\n",
    "#jpeg_files = glob.glob(osp.join(root_path, 'JPEGImages/') + '*.jpg')\n",
    "#png_files = glob.glob(osp.join(root_path, 'SegmentationClass/') + '*_crop.png')\n",
    "\n",
    "jpeg_files.sort()\n",
    "png_files.sort()\n",
    "\n",
    "# Both should equal 36 for v1.0.0 Lab dataset\n",
    "print(len(jpeg_files)) \n",
    "print(len(png_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "colab_type": "code",
    "id": "6tV5s7MngNzo",
    "outputId": "9441fb56-1751-46e5-8ee2-0769902aff70"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "These are the full resolution files that correspond to the 1350 patches of \n",
    "the validation split `val_v100.lmdb`.\"\"\"\n",
    "'''\n",
    "val_mask = png_files[-5:]\n",
    "val_jpeg = jpeg_files[-5:]\n",
    "val_jpeg\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Set to True to save the model predictions in PNG format, \n",
    "otherwise proceed to predict biomass without saving images\"\"\"\n",
    "SAVE_PREDICTIONS = True\n",
    "\n",
    "if SAVE_PREDICTIONS:\n",
    "    prediction_path = ''\n",
    "    for t in root.split('/')[:-1]:\n",
    "        prediction_path += t + '/'\n",
    "\n",
    "    prediction_path = osp.join(prediction_path, 'predictions')\n",
    "\n",
    "    if not osp.exists(prediction_path):\n",
    "        os.mkdir(prediction_path)\n",
    "\n",
    "    # src is the training dataset, tgt is the testing dataset\n",
    "    src = 'trainval_v111'\n",
    "    tgt = 'train_v111'\n",
    "    #tgt = '2018-06'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prediction_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a8f45669fb68>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprediction_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'prediction_path' is not defined"
     ]
    }
   ],
   "source": [
    "#prediction_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fontsize = 16\n",
    "\n",
    "left = 0.02  # the left side of the subplots of the figure\n",
    "right = 0.98   # the right side of the subplots of the figure\n",
    "bottom = 0.05  # the bottom of the subplots of the figure\n",
    "top = 0.95     # the top of the subplots of the figure\n",
    "wspace = 0.15  # the amount of width reserved for space between subplots,\n",
    "# expressed as a fraction of the average axis width\n",
    "hspace = 0.1  # the amount of height reserved for space between subplots,\n",
    "# expressed as a fraction of the average axis height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sx, sy, w = 1500, 680, 1100\n",
    "sx, sy, w = 1250, 200, 1550\n",
    "\n",
    "#plt.imshow(imgc[sy:sy+w, sx:sx+w, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jpeg_files[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#jpeg_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "cxaxZTfigNzs",
    "outputId": "394b6282-16e5-4596-dd56-4272301189a4",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#for i in range(len(val_jpeg)):\n",
    "i = 4\n",
    "\n",
    "image_stem = jpeg_files[i].split('/')[-1].split('.')[0]\n",
    "\n",
    "bgr_lab = cv2.imread(osp.join(root_path, png_files[i]))\n",
    "labc = cv2.cvtColor(bgr_lab, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "bgr_img = cv2.imread(osp.join(root_path, jpeg_files[i]))\n",
    "imgc = cv2.cvtColor(bgr_img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "imgc = imgc[sy:sy+w, sx:sx+w, :]\n",
    "labc = labc[sy:sy+w, sx:sx+w, :]\n",
    "\n",
    "image = imgc.copy()\n",
    "\n",
    "# pre-processing image consistent with PyTorch training transforms\n",
    "imgc = imgc / 255\n",
    "imgc = ((imgc - np.array([0.5, 0.5, 0.5])) / np.array([0.5, 0.5, 0.5]))\n",
    "\n",
    "imgt = torch.FloatTensor(imgc).to(device)\n",
    "imgt = imgt.unsqueeze(0)\n",
    "\n",
    "# Note: need to call contigious after the permute \n",
    "# else max pooling will fail\n",
    "nchw_tensor = imgt.permute(0, 3, 1, 2).contiguous()\n",
    "\n",
    "with torch.no_grad():\n",
    "    pred = sig(net(nchw_tensor))\n",
    "pred_np = pred.detach().cpu().numpy().squeeze()\n",
    "\n",
    "# OpenCV loads the PNG mask as indexed color RGB, \n",
    "# we need to convert it to a binary mask. \n",
    "# The `0' in labc[:, :, 0] is the R channel.\n",
    "mask = np.zeros((labc.shape[0], labc.shape[1]), dtype='float32')\n",
    "mask[labc[:, :, 0] == 128] = 1    \n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.jaccard_score.html\n",
    "\n",
    "#jaccard_fcn = jsc(pred_np.round().reshape(-1, 1), mask.reshape(-1, 1))\n",
    "\n",
    "print('Image %d of %d, IoU %.4f' % (i, len(jpeg_files), jaccard_fcn))\n",
    "\n",
    "#image = cv2.cvtColor(bgr_img[sy:sy+w, sx:sx+w, :], cv2.COLOR_BGR2RGB)\n",
    "\n",
    "#fig, axes = plt.subplots(1, 1, figsize=(10, 4))\n",
    "fig, axes = plt.subplots(1, 1, figsize=(10, 10))\n",
    "#axes = axes.flatten()\n",
    "#axes[0].imshow(image)\n",
    "#axes[0].set_title('Input', fontsize=fontsize)\n",
    "axes.imshow(image, alpha=0.75)\n",
    "axes.imshow(pred_np, alpha=0.5)\n",
    "#axes[1].set_title('Input \\& Preds, IoU = %.4f' % jaccard_fcn, fontsize=fontsize)\n",
    "#axes[2].imshow(mask)\n",
    "#axes[2].set_title('Ground Truth Segmentation', fontsize=fontsize)\n",
    "#plt.subplots_adjust(left=left, bottom=bottom, right=right, top=top, wspace=wspace, hspace=hspace)\n",
    "#pretty_image(axes)\n",
    "#plt.tight_layout()\n",
    "\n",
    "'''\n",
    "if SAVE_PREDICTIONS:\n",
    "    #filename = src + '-' + tgt + '__' + image_stem + '_patch_width%d' % w + '__' + model_stem\n",
    "    filename = src + '-' + tgt + '__' + image_stem + '__' + model_stem\n",
    "    out_file = osp.join(prediction_path, filename)\n",
    "    fig.savefig(out_file + '.png', format='png')\n",
    "    fig.savefig(out_file + '.eps', format='eps')\n",
    "'''   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(pred_np.round())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#jsc(pred_np.round().reshape(1, -1), mask.reshape(1, -1), average='samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = (pred_np.round() * 255).astype('uint8')\n",
    "\n",
    "# noise removal\n",
    "kernel = np.ones((3, 3), np.uint8)\n",
    "opening = cv2.morphologyEx(thresh, cv2.MORPH_OPEN, kernel, iterations = 2)\n",
    "\n",
    "# sure background area\n",
    "sure_bg = cv2.dilate(opening, kernel, iterations=3)\n",
    "\n",
    "# Finding sure foreground area\n",
    "dist_transform = cv2.distanceTransform(opening, cv2.DIST_L2, 5)\n",
    "\n",
    "ret, sure_fg = cv2.threshold(dist_transform, 0.5 * dist_transform.max(), 255, 0)\n",
    "\n",
    "# Finding unknown region\n",
    "sure_fg = np.uint8(sure_fg)\n",
    "unknown = cv2.subtract(sure_bg,sure_fg)\n",
    "\n",
    "# Marker labelling\n",
    "ret, markers = cv2.connectedComponents(sure_fg)\n",
    "\n",
    "# Add one to all labels so that sure background is not 0, but 1\n",
    "markers = markers + 1\n",
    "\n",
    "# Now, mark the region of unknown with zero\n",
    "markers[unknown == 255] = 0\n",
    "\n",
    "markers = cv2.watershed(image, markers)\n",
    "image[markers == -1] = [0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_fname = jpeg_files[i].split('/')[-1].split('.')[0].split('_image')[0].split('Lab_')[1]\n",
    "guid = image_df[image_df['Name'].str.contains(root_fname)]['Analysis Index'].astype('int64')\n",
    "row = data_df[data_df['Analysis Index'].values == np.unique(guid.values)]\n",
    "print(row['Biomass'].values)\n",
    "print(row['Count'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(11.00, 11.00))\n",
    "plt.imshow(markers, alpha=0.5)\n",
    "#plt.imshow(image, alpha=0.5)\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "#plt.savefig(image_stem + '_watershed_actual_count%d_pcount%d.png' % (row['Count'].values, pcount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals, cts = np.unique(markers, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcount = vals[-1]\n",
    "pcount\n",
    "\n",
    "#plt.hist(cts[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cts[2:]\n",
    "\n",
    "# for 3554-1\n",
    "# ([ 6992, 10533,  7258, 12317,  9780,  8321, 15397,  6976,  6130, 4563, 11418,  8436,  7211, 10568,  7740])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bonus = 0\n",
    "div = 7500.\n",
    "for v in cts[2:][cts[2:] > div]:\n",
    "    bonus += np.floor(v / div)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bonus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#30 + 78"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_mussels(image, thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.hist(cts[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_mussels(image, predictions):\n",
    "    \"\"\" Counts mussels in predicted output.\n",
    "    \n",
    "    @param predictions: greyscale predictions as float in [0, 1]\n",
    "    \"\"\"\n",
    "    #thresh = (predictions * 255).astype('uint8')\n",
    "\n",
    "    # noise removal\n",
    "    kernel = np.ones((3, 3), np.uint8)\n",
    "    opening = cv2.morphologyEx(predictions, cv2.MORPH_OPEN, kernel, iterations = 2)\n",
    "\n",
    "    # sure background area\n",
    "    sure_bg = cv2.dilate(opening, kernel, iterations=3)\n",
    "\n",
    "    # Finding sure foreground area\n",
    "    dist_transform = cv2.distanceTransform(opening, cv2.DIST_L2, 5)\n",
    "\n",
    "    ret, sure_fg = cv2.threshold(dist_transform, 0.4 * dist_transform.max(), 255, 0)\n",
    "\n",
    "    # Finding unknown region\n",
    "    sure_fg = np.uint8(sure_fg)\n",
    "    unknown = cv2.subtract(sure_bg,sure_fg)\n",
    "\n",
    "    # Marker labelling\n",
    "    ret, markers = cv2.connectedComponents(sure_fg)\n",
    "\n",
    "    # Add one to all labels so that sure background is not 0, but 1\n",
    "    markers = markers + 1\n",
    "\n",
    "    # Now, mark the region of unknown with zero\n",
    "    markers[unknown == 255] = 0\n",
    "\n",
    "    markers = cv2.watershed(image, markers)\n",
    "    #image[markers == -1] = [255, 0, 0]\n",
    "    \n",
    "    vals, cts = np.unique(markers, return_counts=True)\n",
    "    '''\n",
    "    bonus = 0\n",
    "    div = 7500.\n",
    "    for v in cts[2:][cts[2:] > div]:\n",
    "        bonus += np.floor(v / div)\n",
    "    '''\n",
    "    \n",
    "    return vals[-1] # + bonus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lDACpG6UgNzz"
   },
   "source": [
    "### 7. ii) Refine the Predictions with Post-Processing by CRF\n",
    "\n",
    "Notice the sand in the middle of the image index `i=0` which is initially prediced as mussel. The CRF excels at \n",
    "suppressing such false positives and mIoU increases by 10 pts. Unfortunately it introduces some spurious detection \n",
    "of grid lines, thus doesn't help on all images. Meta-parameters of the CRF can be tuned further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "69VutcNqgNz4",
    "outputId": "ebb2560e-eece-4093-9836-ef0961bb1c61"
   },
   "outputs": [],
   "source": [
    "pred_crf = run_crf(image, pred_np)\n",
    "jaccard_crf = jsc(pred_crf.reshape(-1, 1), mask.reshape(-1, 1))\n",
    "print('CRF IoU %.4f' % jaccard_crf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(10, 4))\n",
    "axes = axes.flatten()\n",
    "axes[0].imshow(image)\n",
    "axes[0].set_title('Input', fontsize=fontsize)\n",
    "axes[1].imshow(image, alpha=0.75)\n",
    "axes[1].imshow(pred_np.round(), alpha=0.5)\n",
    "axes[1].set_title('FCN Preds, IoU = %.4f' % jaccard_fcn, fontsize=fontsize)\n",
    "\n",
    "axes[2].imshow(image, alpha=0.75)\n",
    "axes[2].imshow(pred_crf, alpha=0.5)\n",
    "axes[2].set_title('Post CRF Preds, IoU = %.4f' % jaccard_crf, fontsize=fontsize)\n",
    "\n",
    "plt.subplots_adjust(left=left, bottom=bottom, right=right, top=top, wspace=wspace, hspace=hspace)\n",
    "pretty_image(axes)\n",
    "\n",
    "if SAVE_PREDICTIONS:\n",
    "    filename = src + '-' + tgt + '__' + image_stem + '_patch_width%d_crf' % w + '__' + model_stem\n",
    "    #filename = src + '-' + tgt + '__' + image_stem + '__' + model_stem\n",
    "    out_file = osp.join(prediction_path, filename)\n",
    "    fig.savefig(out_file + '.png', format='png')\n",
    "    fig.savefig(out_file + '.eps', format='eps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 734
    },
    "colab_type": "code",
    "id": "8Sw4tggogNz7",
    "outputId": "b6a42a2e-28ba-43f0-cf3d-81109e565c38"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(12, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "axes[0].imshow(image)\n",
    "axes[0].set_title('Input')\n",
    "\n",
    "axes[1].imshow(mask)\n",
    "axes[1].set_title('Ground Truth Segmentation', fontsize=fontsize)\n",
    "\n",
    "axes[2].imshow(image, alpha=0.75)\n",
    "axes[2].imshow(pred_np.round(), alpha=0.5)\n",
    "axes[2].set_title('Input \\& Soft Preds, IoU = %.4f' % jaccard_fcn, fontsize=fontsize)\n",
    "\n",
    "axes[3].imshow(image, alpha=0.75)\n",
    "axes[3].imshow(pred_crf, alpha=0.5)\n",
    "axes[3].set_title('Input \\& CRF Preds, IoU = %.4f' % jaccard_crf, fontsize=fontsize)\n",
    "\n",
    "pretty_image(axes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9h2AhUsjgNz-"
   },
   "source": [
    "## 8. Predict Mussel Biomass\n",
    "\n",
    "Here we predict the mussel biomass from the lab analysis using a) the masks, and b) model predictions on the \n",
    "full size images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vYuNMNd9gNz_"
   },
   "outputs": [],
   "source": [
    "if not IN_COLAB:\n",
    "    DATA_PATH = r'/scratch/gallowaa/cciw/Data'\n",
    "\n",
    "imagetable_path = os.path.join(DATA_PATH, 'Tables', 'ImageTable.csv')\n",
    "image_df = pd.read_csv(imagetable_path, index_col=0)\n",
    "\n",
    "analysis_path = os.path.join(DATA_PATH, 'Tables', 'Analysis.csv')\n",
    "dive_path = os.path.join(DATA_PATH, 'Tables', 'Dives.csv')\n",
    "\n",
    "analysis_df = pd.read_csv(analysis_path, index_col=0, dtype={'Count':float})\n",
    "dive_df = pd.read_csv(dive_path, index_col=0, parse_dates=['Date'])\n",
    "data_df = pd.merge(analysis_df, dive_df, on='Dive Index', how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CfUSnfsugN0D"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "numpy array with manually estimated camera distance based on counting \n",
    "squares along horizontal and vertical axes of each Lab image.\n",
    "\n",
    "Useful to determine how much performance can be gained by accounting \n",
    "for camera distance programmatically.\"\"\"\n",
    "scale = np.load('lab_board_dims_n40.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relates to Deliverable 2. c) *Predicted semantic segmentation (mussel/no-mussel) for all images in 2019 testing set in png image format.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "8bwN2IhcgN0G",
    "outputId": "06676eec-0c7e-4733-887d-c47a7b211f9c"
   },
   "outputs": [],
   "source": [
    "lab_ct = []  # for storing the number of mussel pixels in each mask\n",
    "prd_ct = []  # for storing the number of mussel pixels in each prediction\n",
    "\n",
    "# This cell is slow because we're randomly reading large images from Google Drive\n",
    "for i in tqdm(range(len(jpeg_files)), unit=' image'):\n",
    "    \n",
    "    bgr_img = cv2.imread(osp.join(root_path, jpeg_files[i]))\n",
    "    #bgr_lab = cv2.imread(osp.join(root_path, png_files[i]))\n",
    "    \n",
    "    #_, cts = np.unique(bgr_lab, return_counts=True)\n",
    "    #lab_ct.append(cts[1] / cts.sum())    \n",
    "    \n",
    "    img = cv2.cvtColor(bgr_img, cv2.COLOR_BGR2RGB)\n",
    "    #lab = cv2.cvtColor(bgr_lab, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    image = img.copy()\n",
    "\n",
    "    # pre-processing image consistent with PyTorch training transforms\n",
    "    img = img / 255.\n",
    "    img = ((img - np.array([0.5, 0.5, 0.5])) / np.array([0.5, 0.5, 0.5]))\n",
    "\n",
    "    imgt = torch.FloatTensor(img).to(device)\n",
    "    imgt = imgt.unsqueeze(0)\n",
    "\n",
    "    # Note: need to call contigious after the permute \n",
    "    # else max pooling will fail\n",
    "    nchw_tensor = imgt.permute(0, 3, 1, 2).contiguous()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred = sig(net(nchw_tensor))\n",
    "    \n",
    "    pred_np = pred.squeeze().round().detach().cpu().numpy()\n",
    "    prd_ct.append(count_mussels(image, (pred_np * 255).astype('uint8') ))\n",
    "    \n",
    "    #grey_mask = np.zeros((lab.shape[0], lab.shape[1]), dtype='uint8')\n",
    "    #grey_mask[lab[:, :, 0] == 128] = 255\n",
    "    #lab_ct.append(count_mussels(image, grey_mask))\n",
    "    \n",
    "    #prd_ct.append(pred.round().sum().item() / cts.sum())\n",
    "\n",
    "    if SAVE_PREDICTIONS:\n",
    "        '''\n",
    "        prediction = (pred.squeeze().round() * 255)\n",
    "        prediction_np = prediction.detach().cpu().numpy().astype('uint8')\n",
    "        out_file = osp.join(prediction_path, \n",
    "                            jpeg_files[i].split('/')[-1].split('.')[0] + '_' + ckpt_file.split('.')[0] + '.png')\n",
    "        cv2.imwrite(out_file, prediction_np)\n",
    "        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ioB337i5gN0J"
   },
   "outputs": [],
   "source": [
    "CORRECT_CAMERA_DISTANCE = True\n",
    "\n",
    "#lab_ct_np = np.asarray(lab_ct)\n",
    "prd_ct_np = np.asarray(prd_ct)\n",
    "prd_ct_np_cam = np.zeros_like(prd_ct_np)\n",
    "\n",
    "lab_targets = np.zeros((len(jpeg_files), 3)) # 0 = biomass, 1 = count\n",
    "\n",
    "names = ['16mm', '14mm', '12.5mm', '10mm', '8mm', '6.3mm', '4mm', '2mm']\n",
    "sieves = np.array([16, 14, 12.5, 10, 8, 6.3, 4, 2])\n",
    "\n",
    "for i in range(len(jpeg_files)):\n",
    "    \n",
    "    # adjust the pixel_ct by size of grid (16 squares high, 25 wide)\n",
    "    if CORRECT_CAMERA_DISTANCE:\n",
    "        #lab_ct_np[i] = lab_ct_np[i] * (np.prod(scale[i]) / (16 * 25))\n",
    "        prd_ct_np_cam[i] = prd_ct_np[i] * (np.prod(scale[i]) / (16 * 25))\n",
    "    \n",
    "    '''\n",
    "    if 'scale' in png_files[i]:\n",
    "        root_fname = png_files[i].split('/')[-1].split('.')[0].split('_scale')[0][4:-8]\n",
    "    else:\n",
    "        root_fname = png_files[i].split('/')[-1].split('.')[0].split('_mask')[0][4:-8]\n",
    "    ''' \n",
    "    root_fname = jpeg_files[i].split('/')[-1].split('.')[0].split('_image')[0].split('Lab_')[1]\n",
    "    \n",
    "    guid = image_df[image_df['Name'].str.contains(root_fname)]['Analysis Index'].astype('int64')\n",
    "    row = data_df[data_df['Analysis Index'].values == np.unique(guid.values)]\n",
    "    lab_targets[i, 0] = row['Biomass'].values\n",
    "    lab_targets[i, 1] = row['Count'].values\n",
    "    \n",
    "    size_dist = np.zeros(len(names))\n",
    "    for j in range(len(names)):\n",
    "        size_dist[j] = row[names[j]].values\n",
    "        \n",
    "    #lab_targets[i, 2] = (lab_targets[i, 0] * size_dist * (2 / sieves)**(1/3)).sum()\n",
    "        \n",
    "#lab_ct_np = lab_ct_np / lab_ct_np.max()    \n",
    "#prd_ct_np = prd_ct_np / prd_ct_np.max()\n",
    "#prd_ct_np_cam = prd_ct_np_cam / prd_ct_np_cam.max()\n",
    "\n",
    "lab_targets[np.isnan(lab_targets)] = 0\n",
    "#y = lab_targets[:, 1] / lab_targets[:, 1].max()\n",
    "#yc = lab_targets[:, 2] / lab_targets[:, 2].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = lab_targets[:, 1]\n",
    "t = 1500\n",
    "\n",
    "inliers = y < t\n",
    "outlier = (y > 1390) & (prd_ct_np < 300)\n",
    "inliers = inliers & np.invert(outlier)\n",
    "\n",
    "prd_ct_np = prd_ct_np[inliers]\n",
    "prd_ct_np_cam = prd_ct_np_cam[inliers]\n",
    "y = y[inliers]\n",
    "\n",
    "prd_ct_np = prd_ct_np / prd_ct_np.max()\n",
    "prd_ct_np_cam = prd_ct_np_cam / prd_ct_np_cam.max()\n",
    "y = y / y.max()\n",
    "\n",
    "#plt.scatter(prd_ct_np[inliers], lab_targets[:, 1][inliers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(prd_ct_np, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BdpOKTgorNqW"
   },
   "source": [
    "Finally, plot biomass versus pixels predicted as mussel. Interestingly, the \n",
    "model **predictions** outperform the **masks** in terms of accounting for \n",
    "variance in biomass. This holds both when `CORRECT_CAMERA_DISTANCE=True` or `=False`.\n",
    "\n",
    "This is likely due to a CLT-style smoothing effect, or the model paying \"equal \n",
    "attention\" to all images, whereas the Lab images were labelled by different \n",
    "people (myself and Scale) and likely have idiosyncrasies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fontsize = 16\n",
    "\n",
    "left = 0.02  # the left side of the subplots of the figure\n",
    "right = 0.98   # the right side of the subplots of the figure\n",
    "bottom = 0.05  # the bottom of the subplots of the figure\n",
    "top = 0.95     # the top of the subplots of the figure\n",
    "wspace = 0.15  # the amount of width reserved for space between subplots,\n",
    "# expressed as a fraction of the average axis width\n",
    "hspace = 0.1  # the amount of height reserved for space between subplots,\n",
    "# expressed as a fraction of the average axis height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_count_1x2(x_data_1, x_data_2, y_data, x1_label='', x2_label=''):\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 2, figsize=(8, 4), sharex=True, sharey=True)\n",
    "\n",
    "    ax[0].scatter(x_data_1, y_data, marker='o', s=40, facecolors='none', edgecolors='k')\n",
    "    ax[0].set_ylabel('Count', fontsize=fontsize)\n",
    "    ax[0].set_xlabel(x1_label, fontsize=fontsize)\n",
    "    \n",
    "    ax[1].scatter(x_data_2, y_data, marker='o', s=40, facecolors='none', edgecolors='k')\n",
    "    ax[1].set_xlabel(x2_label, fontsize=fontsize)\n",
    "    \n",
    "    draw_lines(ax[0], x_data_1, y_data)\n",
    "    draw_lines(ax[1], x_data_2, y_data)\n",
    "    \n",
    "    #draw_rsquared(ax[0], x_data_1, y_data, fontsize)\n",
    "    #draw_rsquared(ax[1], x_data_2, y_data, fontsize)\n",
    "    draw_rsquared(ax[0], y_data, x_data_1, fontsize)\n",
    "    draw_rsquared(ax[1], y_data, x_data_2, fontsize)\n",
    "    \n",
    "    \n",
    "    draw_sublabel(ax[0], r'\\textbf{a)}', fontsize)\n",
    "    draw_sublabel(ax[1], r'\\textbf{b)}', fontsize)\n",
    "    \n",
    "    pretty_axis(ax[0], fontsize)\n",
    "    pretty_axis(ax[1], fontsize)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_count_1x2(\n",
    "    prd_ct_np, prd_ct_np_cam, y, \n",
    "    x1_label='Prediction',\n",
    "    x2_label='Prediction \\n (camera corrected)')\n",
    "\n",
    "fname = 'lab_predict_count_' + src + '-' + tgt + '__' + model_stem\n",
    "fig.savefig(fname + '.png')\n",
    "fig.savefig(fname + '.eps', format='eps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "colab_type": "code",
    "id": "IA-2nitYgN0N",
    "outputId": "1bbca420-004a-44b6-c41c-78ffccfc46f1"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(8, 4), sharex=True, sharey=True)\n",
    "\n",
    "ax[0].scatter(prd_ct_np, y, marker='o', s=40, facecolors='none', edgecolors='b')\n",
    "ax[1].scatter(prd_ct_np_cam, y, c='k') #, marker='o', s=40, facecolors='none', edgecolors='k')\n",
    "\n",
    "#ax[0].set_ylabel('Mussel Biomass (grams)', fontsize=fontsize)\n",
    "ax[0].set_ylabel('Count', fontsize=fontsize)\n",
    "ax[0].set_ylim(0, 1.05)\n",
    "ax[0].set_xlim(0, 1.05)\n",
    "ax[0].set_xlabel('Fraction of Mussel Pixels \\n (Mask)', fontsize=fontsize)\n",
    "ax[1].set_xlabel('Fraction of Mussel Pixels \\n (Prediction)', fontsize=fontsize)\n",
    "ax[0].tick_params(labelsize=fontsize-2)\n",
    "ax[1].tick_params(labelsize=fontsize-2)\n",
    "\n",
    "x = np.linspace(0, 1)\n",
    "\n",
    "#A = np.vstack([lab_ct_np, np.ones(len(lab_ct_np))]).T\n",
    "#m, c = np.linalg.lstsq(A, y, rcond=None)[0]\n",
    "#ax[0].plot(x, m*x + c, 'b', linestyle='-', label='masks')\n",
    "\n",
    "A = np.vstack([prd_ct_np, np.ones(len(prd_ct_np))]).T\n",
    "m, c = np.linalg.lstsq(A, y, rcond=None)[0]\n",
    "ax[1].plot(x, m*x + c, 'k', linestyle='--', label='preds')\n",
    "\n",
    "ax[0].annotate(r'$\\mathbf{R^2}$ = %.4f' % r2_score(y, prd_ct_np), \n",
    "            xy=(.05, .85), fontsize=fontsize + 1, xycoords='axes fraction', color='b')\n",
    "\n",
    "ax[1].annotate(r'$\\mathbf{R^2}$ = %.4f' % r2_score(y, prd_ct_np_cam), \n",
    "            xy=(.05, .85), fontsize=fontsize + 1, xycoords='axes fraction', color='k')\n",
    "\n",
    "ax[0].grid()\n",
    "ax[1].grid()\n",
    "\n",
    "#ax[0].legend(loc='lower right', fontsize=fontsize-2)\n",
    "#ax[1].legend(loc='lower right', fontsize=fontsize-2)\n",
    "\n",
    "ax[0].set_aspect('equal')\n",
    "ax[1].set_aspect('equal')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tmp = \n",
    "out = (y > 0.95) & (prd_ct_np < 0.6)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#jpeg_files[np.argmax(out)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y < 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rkBPnFLogN0S"
   },
   "source": [
    "### Optionally save the plot as png or vector graphic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I8G_zejAgN0T"
   },
   "outputs": [],
   "source": [
    "fname = 'lab_predict_biomass_from_pixels_no_camera' + src + '-' + tgt + '__' + model_stem\n",
    "fig.savefig(fname + '.png')\n",
    "fig.savefig(fname + '.eps', format='eps')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of current demo\n",
    "\n",
    "__ToDo:__ CSV file containing predicted (i) percentage coverage, (ii) total mussels count, (iii) total\n",
    "mussels biomass and (iv) mussels size distribution with error estimates for each image\n",
    "acquired in 2019. \n",
    "\n",
    "To do after troubleshooting performance on the *in situ* dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to set the path to the full size images and labels on the Google Drive\n",
    "root_path = '/scratch/ssd/gallowaa/cciw/VOCdevkit/Train-v111-originals/'\n",
    "\n",
    "label_path = os.path.join(root_path, 'SegmentationClass')\n",
    "image_path = os.path.join(root_path, 'JPEGImages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for all png label files\n",
    "#all_images = glob.glob(osp.join(label_path, '*.png'))\n",
    "\n",
    "jpeg_files = glob.glob(osp.join(image_path, '*.jpg'))\n",
    "png_files = glob.glob(osp.join(label_path, '*.png'))\n",
    "\n",
    "print(len(jpeg_files))\n",
    "print(len(png_files))\n",
    "\n",
    "jpeg_files.sort()\n",
    "png_files.sort()\n",
    "\n",
    "# show the first few files\n",
    "png_files[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_targets = np.zeros((len(png_files), 3)) # 0 = biomass, 1 = count\n",
    "\n",
    "for i in range(len(png_files)):\n",
    "    key = png_files[i].split('/')[-1].split('.')[0].split('_image')[0]\n",
    "    guid = image_df[image_df['Name'].str.contains(key)]['Analysis Index'].astype('int64')\n",
    "    row = data_df[data_df['Analysis Index'].values == np.unique(guid.values)]\n",
    "    \n",
    "    lab_targets[i, 0] = row['Count'].values\n",
    "    lab_targets[i, 1] = row['Biomass'].values\n",
    "    lab_targets[i, 2] = row['Live Coverage'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pix_ct = []\n",
    "for i in tqdm(range(len(png_files))):\n",
    "    im   = cv2.imread(png_files[i])\n",
    "    _, cts = np.unique(im, return_counts=True) \n",
    "    try:\n",
    "        pix_ct.append(cts[1] / cts.sum())\n",
    "    except:\n",
    "        pix_ct.append(0)\n",
    "pix_ct_np = np.asarray(pix_ct)\n",
    "pix_ct_np = pix_ct_np / pix_ct_np.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prd_ct = []  # for storing the number of mussel pixels in each prediction\n",
    "\n",
    "for i in tqdm(range(len(jpeg_files)), unit=' image'):\n",
    "    \n",
    "    bgr_img = cv2.imread(jpeg_files[i])\n",
    "    img = cv2.cvtColor(bgr_img, cv2.COLOR_BGR2RGB)\n",
    "    bgr_lab   = cv2.imread(png_files[i])\n",
    "    lab = cv2.cvtColor(bgr_lab, cv2.COLOR_BGR2RGB)\n",
    "    image = img.copy()\n",
    "\n",
    "    # pre-processing image consistent with PyTorch training transforms\n",
    "    img = img / 255.\n",
    "    img = ((img - np.array([0.5, 0.5, 0.5])) / np.array([0.5, 0.5, 0.5]))\n",
    "\n",
    "    imgt = torch.FloatTensor(img).to(device)\n",
    "    imgt = imgt.unsqueeze(0)\n",
    "\n",
    "    # Note: need to call contigious after the permute \n",
    "    # else max pooling will fail\n",
    "    nchw_tensor = imgt.permute(0, 3, 1, 2).contiguous()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred = sig(net(nchw_tensor))\n",
    "    \n",
    "    pred_np = pred.squeeze().round().detach().cpu().numpy()\n",
    "    #prd_ct.append(count_mussels(image, (pred_np * 255).astype('uint8') ))\n",
    "    \n",
    "    #grey_mask = np.zeros((lab.shape[0], lab.shape[1]), dtype='uint8')\n",
    "    #grey_mask[lab[:, :, 0] == 128] = 255\n",
    "    #lab_ct.append(count_mussels(image, grey_mask))\n",
    "    \n",
    "    train_prd_ct.append(pred.round().sum().item() / cts.sum())\n",
    "\n",
    "    if SAVE_PREDICTIONS:\n",
    "        \n",
    "        prediction = (pred.squeeze().round() * 255)\n",
    "        prediction_np = prediction.detach().cpu().numpy().astype('uint8')\n",
    "        image_stem = jpeg_files[i].split('/')[-1].split('.')[0]\n",
    "        out_file = osp.join(prediction_path, src + '-' + tgt + '__' + image_stem + '_' + ckpt_file.split('.')[0] + '.png')\n",
    "        \n",
    "        #print(out_file)\n",
    "        #cv2.imwrite(out_file, prediction_np)\n",
    "        \n",
    "        plt.close('all')\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(20, 8))\n",
    "        axes = axes.flatten()\n",
    "        axes[0].imshow(image)\n",
    "        axes[0].set_title('Input', fontsize=fontsize)\n",
    "        axes[1].imshow(image, alpha=0.75)\n",
    "        axes[1].imshow(prediction_np, alpha=0.5)\n",
    "        axes[1].set_title('Input \\& Preds', fontsize=fontsize)\n",
    "        axes[2].imshow(lab)\n",
    "        #axes[2].set_title('Post CRF Preds, IoU = %.4f' % jaccard_crf, fontsize=fontsize)\n",
    "        plt.subplots_adjust(left=left, bottom=bottom, right=right, top=top, wspace=wspace, hspace=hspace)\n",
    "        pretty_image(axes)\n",
    "        fig.savefig(out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[prediction_np == 0] = 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_file = osp.join(prediction_path, src + '-' + tgt + '__' + image_stem + '_' + ckpt_file.split('.')[0] + '.png')\n",
    "out_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filename = src + '-' + tgt + '__' + image_stem + '__' + model_stem\n",
    "#filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prd_ct_np = np.asarray(train_prd_ct)\n",
    "train_prd_ct_np = train_prd_ct_np / train_prd_ct_np.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_targets[np.isnan(lab_targets)] = 0\n",
    "\n",
    "for i in range(3):\n",
    "    lab_targets[:, i] = lab_targets[:, i] / lab_targets[:, i].max()    \n",
    "\n",
    "x = pix_ct_np.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_y = lab_targets[:, 2] > 0.4\n",
    "mask_x = x < 0.2\n",
    "upper_left = mask_x & mask_y\n",
    "\n",
    "mask_y = lab_targets[:, 2] > 0.38\n",
    "mask_x = x < 0.1\n",
    "upper_left |= (mask_x & mask_y)\n",
    "\n",
    "mask_y = lab_targets[:, 2] < 0.6\n",
    "mask_x = x > 0.6\n",
    "bottom_right = mask_x & mask_y\n",
    "\n",
    "outliers = upper_left | bottom_right\n",
    "\n",
    "inliers = np.invert(outliers)\n",
    "print('Live Coverage R^2 value on %d inliers = %.4f' % (len(x[inliers]), r2_score(lab_targets[:, 2][inliers], x[inliers])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xin = x[inliers]\n",
    "train_prd_ct_np_in = train_prd_ct_np[inliers]\n",
    "livein = lab_targets[:, 2][inliers]\n",
    "\n",
    "xin = xin / xin.max()\n",
    "livein = livein / livein.max()\n",
    "train_prd_ct_np_in = train_prd_ct_np_in / train_prd_ct_np_in.max()\n",
    "\n",
    "print(len(train_prd_ct_np_in))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(8, 4), sharex=True, sharey=True)\n",
    "\n",
    "ax[0].scatter(xin, lab_targets[:, 1][inliers], marker='o', s=40, facecolors='none', edgecolors='k')\n",
    "ax[0].set_ylabel('Biomass (g)', fontsize=fontsize)\n",
    "ax[0].set_xlabel('Fraction of Mussel Pixels \\n (Mask)', fontsize=fontsize)\n",
    "\n",
    "ax[1].scatter(train_prd_ct_np_in, lab_targets[:, 1][inliers], marker='o', s=40, facecolors='none', edgecolors='k')\n",
    "ax[1].set_xlabel('Fraction of Mussel Pixels \\n (Mask)', fontsize=fontsize)\n",
    "\n",
    "draw_lines(ax[0], xin, lab_targets[:, 1][inliers])\n",
    "draw_lines(ax[1], train_prd_ct_np_in, lab_targets[:, 1][inliers])\n",
    "\n",
    "draw_rsquared(ax[0], lab_targets[:, 1][inliers], xin, fontsize)\n",
    "draw_rsquared(ax[1], lab_targets[:, 1][inliers], train_prd_ct_np_in, fontsize)\n",
    "\n",
    "draw_sublabel(ax[0], r'\\textbf{a)}', fontsize)\n",
    "draw_sublabel(ax[1], r'\\textbf{b)}', fontsize)\n",
    "\n",
    "pretty_axis(ax[0], fontsize)\n",
    "pretty_axis(ax[1], fontsize)\n",
    "\n",
    "plt.tight_layout()\n",
    "fname = 'train_v111_biomass_from_masks'\n",
    "#fig.savefig(fname + '.png')\n",
    "#fig.savefig(fname + '.eps', format='eps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(8, 4), sharex=True, sharey=True)\n",
    "\n",
    "ax[0].scatter(xin, livein, marker='o', s=40, facecolors='none', edgecolors='k')\n",
    "ax[0].set_ylabel('Live Coverage (\\%)', fontsize=fontsize)\n",
    "ax[0].set_xlabel('Fraction of Mussel Pixels \\n (Mask)', fontsize=fontsize)\n",
    "\n",
    "ax[1].scatter(train_prd_ct_np_in, livein, marker='o', s=40, facecolors='none', edgecolors='k')\n",
    "ax[1].set_xlabel('Fraction of Mussel Pixels \\n (Prediction)', fontsize=fontsize)\n",
    "\n",
    "draw_lines(ax[0], xin, livein)\n",
    "draw_lines(ax[1], train_prd_ct_np_in, livein)\n",
    "\n",
    "#draw_rsquared(ax[0], livein, xin, fontsize)\n",
    "#draw_rsquared(ax[1], livein, train_prd_ct_np_in, fontsize)\n",
    "\n",
    "ax[0].set_title(r'$\\mathbf{R^2}$ = %.4f' % r2_score(livein, xin), fontsize=fontsize + 1)\n",
    "ax[1].set_title(r'$\\mathbf{R^2}$ = %.4f' % r2_score(livein, train_prd_ct_np_in), fontsize=fontsize + 1)\n",
    "\n",
    "draw_sublabel(ax[0], r'\\textbf{a)}', fontsize)\n",
    "draw_sublabel(ax[1], r'\\textbf{b)}', fontsize)\n",
    "\n",
    "pretty_axis(ax[0], fontsize)\n",
    "pretty_axis(ax[1], fontsize)\n",
    "\n",
    "plt.tight_layout()\n",
    "fname = 'train_v111_live_coverage_from_masks'\n",
    "fig.savefig(fname + '.png')\n",
    "fig.savefig(fname + '.eps', format='eps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adversarial examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a black image\n",
    "img = np.zeros((224, 224), np.uint8)\n",
    "sx = 10\n",
    "sy = 5\n",
    "j = 40\n",
    "for i in range(4):\n",
    "    img = cv2.ellipse(img, (j * (i + 1), j * (i + 1)), (sx * (i + 1), sy * (i + 1)), 10, -180, 180, 255, -1)\n",
    "img = (img / 255.).astype('float32')\n",
    "plt.imshow(img)\n",
    "img = np.expand_dims(img, 0)\n",
    "img = np.expand_dims(img, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_target = torch.FloatTensor(img).to(device)\n",
    "adv_target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = torch.randn((1, 3, 224, 224)) / 1000\n",
    "noise = noise.to(device)\n",
    "noise.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise.requires_grad_()\n",
    "noise.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fnct = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(200):\n",
    "    \n",
    "    noise.requires_grad_()\n",
    "    loss = loss_fnct(net(noise), adv_target) * 1e-3 * noise.norm(2)\n",
    "    loss.backward()\n",
    "    loss_grad = noise.grad.data.clone()\n",
    "    #signed_grad = torch.sign(loss_grad)\n",
    "    noise = noise.detach() - (100 * loss_grad).to(device)\n",
    "    print(i, loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz = noise.permute(0, 2, 3, 1).squeeze().detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(viz - viz.min()).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "viz = viz - viz.min()\n",
    "viz = viz / viz.max()\n",
    "plt.imshow(viz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = sig(net(noise))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(pred.squeeze().detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "task_3_evaluate_checkpoint_in_colab.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
